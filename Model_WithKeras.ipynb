{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_WithKeras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/Celestini/blob/master/Model_WithKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB_ovQ-XfhRc",
        "colab_type": "code",
        "outputId": "312bd877-4645-4100-c450-0c648a24e60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "import pickle\n",
        "from sklearn.externals import joblib\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.saved_model import simple_save\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python import pywrap_tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqfmC0FpnwAX",
        "colab_type": "text"
      },
      "source": [
        "The labels and Features files for the model have been listed in the repository to be included here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPlZWUSf04p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "with open('/content/featuresFinal_withNOV.bin','rb') as file:\n",
        "  f = pickle.load(file)\n",
        "  \n",
        "with open('/content/labelsFinal_withNOV.bin','rb') as file:\n",
        "  l = pickle.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYARG24Ts2Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = []\n",
        "labels = []\n",
        "\n",
        "for i in f:\n",
        "  features.append(list(map(float,i)))\n",
        "for i in l:\n",
        "  labels.append(list(map(float,i)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za07w39Et_A4",
        "colab_type": "code",
        "outputId": "6ebb4796-4314-4743-8c2e-e455d65d04a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(np.shape(features))\n",
        "print(np.shape(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2753, 10)\n",
            "(2753, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWX04Xrf5pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_t, x_test_t,y_train_t, y_test_t = train_test_split(features, labels, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKhLYx25gDmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBsJRRwLgUtI",
        "colab_type": "code",
        "outputId": "39e4d63a-a950-47d0-8b70-40c4b77546bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "print(device_lib.list_local_devices())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4123974962877056399\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13364660524813688360\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 18437188615480605122\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11330115994\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 482525017519524655\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U5s3T6kgWmD",
        "colab_type": "code",
        "outputId": "8ed9257c-3ff9-4130-de7a-b765ac783aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "model.add(Dense(15, input_shape = (10, ), activation = 'relu'))\n",
        "model.add(Dense(25, activation = 'relu'))\n",
        "model.add(Dense(1,  activation = 'relu'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy1vyNq3gZGv",
        "colab_type": "code",
        "outputId": "545f36b8-7289-4ded-8a40-1355a32b0eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "\n",
        "model.compile(optimizer = Adam(lr = 0.001),loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 15)                165       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                400       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 591\n",
            "Trainable params: 591\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTkDmTcgbdw",
        "colab_type": "code",
        "outputId": "df804d61-076a-40af-b04a-ba026e40a600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit([x_train_t], [y_train_t], validation_split = 0.33, epochs = 1100,verbose = 1, batch_size = 32, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 1291 samples, validate on 636 samples\n",
            "Epoch 1/1100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1291/1291 [==============================] - 1s 794us/step - loss: 39868.8789 - val_loss: 40575.1530\n",
            "Epoch 2/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 39242.8925 - val_loss: 39656.4410\n",
            "Epoch 3/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 37715.3715 - val_loss: 37530.0690\n",
            "Epoch 4/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 34918.8332 - val_loss: 34228.6890\n",
            "Epoch 5/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 31357.2274 - val_loss: 30970.1475\n",
            "Epoch 6/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 28436.7510 - val_loss: 28892.3025\n",
            "Epoch 7/1100\n",
            "1291/1291 [==============================] - 0s 170us/step - loss: 26988.5355 - val_loss: 27929.7855\n",
            "Epoch 8/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 26085.7617 - val_loss: 27320.5807\n",
            "Epoch 9/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 25369.8651 - val_loss: 26684.3531\n",
            "Epoch 10/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 24652.9927 - val_loss: 25968.2703\n",
            "Epoch 11/1100\n",
            "1291/1291 [==============================] - 0s 175us/step - loss: 23805.7650 - val_loss: 25185.0981\n",
            "Epoch 12/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 22927.2788 - val_loss: 24323.0172\n",
            "Epoch 13/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 21944.7540 - val_loss: 23396.6812\n",
            "Epoch 14/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 20907.5175 - val_loss: 22428.6501\n",
            "Epoch 15/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 19830.6353 - val_loss: 21363.7159\n",
            "Epoch 16/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 18705.5953 - val_loss: 20275.6856\n",
            "Epoch 17/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 17530.9822 - val_loss: 19178.0756\n",
            "Epoch 18/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 16348.9841 - val_loss: 18060.8577\n",
            "Epoch 19/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 15150.5770 - val_loss: 16904.2517\n",
            "Epoch 20/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 13971.0635 - val_loss: 15763.9218\n",
            "Epoch 21/1100\n",
            "1291/1291 [==============================] - 0s 176us/step - loss: 12816.2764 - val_loss: 14623.3189\n",
            "Epoch 22/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 11716.0895 - val_loss: 13572.2575\n",
            "Epoch 23/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 10671.6125 - val_loss: 12549.7575\n",
            "Epoch 24/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 9720.5868 - val_loss: 11640.1867\n",
            "Epoch 25/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 8866.7319 - val_loss: 10735.9689\n",
            "Epoch 26/1100\n",
            "1291/1291 [==============================] - 0s 140us/step - loss: 8084.2259 - val_loss: 9973.4166\n",
            "Epoch 27/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 7368.9359 - val_loss: 9245.1979\n",
            "Epoch 28/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 6752.1121 - val_loss: 8590.7404\n",
            "Epoch 29/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 6146.7237 - val_loss: 7930.6689\n",
            "Epoch 30/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 5609.6459 - val_loss: 7287.5175\n",
            "Epoch 31/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 5093.6017 - val_loss: 6741.9899\n",
            "Epoch 32/1100\n",
            "1291/1291 [==============================] - 0s 172us/step - loss: 4645.6591 - val_loss: 6246.4581\n",
            "Epoch 33/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 4252.0739 - val_loss: 5780.7429\n",
            "Epoch 34/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 3901.9797 - val_loss: 5362.6478\n",
            "Epoch 35/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 3589.2605 - val_loss: 4997.6749\n",
            "Epoch 36/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 3329.4909 - val_loss: 4683.7569\n",
            "Epoch 37/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 3111.7792 - val_loss: 4391.5301\n",
            "Epoch 38/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 2923.1144 - val_loss: 4158.7380\n",
            "Epoch 39/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 2781.2543 - val_loss: 3945.8483\n",
            "Epoch 40/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 2627.7476 - val_loss: 3764.5978\n",
            "Epoch 41/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 2520.5334 - val_loss: 3602.9393\n",
            "Epoch 42/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 2441.2208 - val_loss: 3470.5031\n",
            "Epoch 43/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 2330.2762 - val_loss: 3342.7230\n",
            "Epoch 44/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 2244.5093 - val_loss: 3249.6863\n",
            "Epoch 45/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 2171.6201 - val_loss: 3148.6743\n",
            "Epoch 46/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 2108.5983 - val_loss: 3065.6005\n",
            "Epoch 47/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 2054.5500 - val_loss: 2998.5833\n",
            "Epoch 48/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 2006.7297 - val_loss: 2916.3462\n",
            "Epoch 49/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 1966.5081 - val_loss: 2860.4645\n",
            "Epoch 50/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 1913.7945 - val_loss: 2824.2942\n",
            "Epoch 51/1100\n",
            "1291/1291 [==============================] - 0s 171us/step - loss: 1880.0777 - val_loss: 2762.7201\n",
            "Epoch 52/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 1835.4607 - val_loss: 2721.8511\n",
            "Epoch 53/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 1802.2991 - val_loss: 2690.0779\n",
            "Epoch 54/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 1770.3839 - val_loss: 2635.9134\n",
            "Epoch 55/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1754.4877 - val_loss: 2626.8833\n",
            "Epoch 56/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 1712.0776 - val_loss: 2559.4055\n",
            "Epoch 57/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 1684.5407 - val_loss: 2555.6652\n",
            "Epoch 58/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 1670.7555 - val_loss: 2533.4735\n",
            "Epoch 59/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1649.9988 - val_loss: 2511.4626\n",
            "Epoch 60/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 1624.2980 - val_loss: 2472.5160\n",
            "Epoch 61/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 1605.0172 - val_loss: 2442.1431\n",
            "Epoch 62/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 1579.9789 - val_loss: 2442.9313\n",
            "Epoch 63/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 1565.7980 - val_loss: 2412.8606\n",
            "Epoch 64/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 1540.4606 - val_loss: 2387.7772\n",
            "Epoch 65/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 1519.4129 - val_loss: 2370.6342\n",
            "Epoch 66/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 1511.1611 - val_loss: 2359.0284\n",
            "Epoch 67/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 1491.3729 - val_loss: 2337.4905\n",
            "Epoch 68/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 1478.3562 - val_loss: 2336.0755\n",
            "Epoch 69/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 1457.1647 - val_loss: 2299.5226\n",
            "Epoch 70/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 1445.7752 - val_loss: 2281.9937\n",
            "Epoch 71/1100\n",
            "1291/1291 [==============================] - 0s 167us/step - loss: 1427.6755 - val_loss: 2253.5749\n",
            "Epoch 72/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 1424.8285 - val_loss: 2237.4519\n",
            "Epoch 73/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1405.1979 - val_loss: 2249.5552\n",
            "Epoch 74/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 1384.4236 - val_loss: 2212.6227\n",
            "Epoch 75/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 1380.0607 - val_loss: 2194.2065\n",
            "Epoch 76/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 1364.1587 - val_loss: 2191.4536\n",
            "Epoch 77/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 1373.9576 - val_loss: 2153.8638\n",
            "Epoch 78/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 1333.1439 - val_loss: 2131.6825\n",
            "Epoch 79/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 1322.8560 - val_loss: 2121.0833\n",
            "Epoch 80/1100\n",
            "1291/1291 [==============================] - 0s 139us/step - loss: 1313.4459 - val_loss: 2136.0942\n",
            "Epoch 81/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 1295.1820 - val_loss: 2086.1200\n",
            "Epoch 82/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 1283.3102 - val_loss: 2090.6927\n",
            "Epoch 83/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1273.6325 - val_loss: 2063.6279\n",
            "Epoch 84/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 1270.7590 - val_loss: 2069.0464\n",
            "Epoch 85/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 1253.5121 - val_loss: 2037.8856\n",
            "Epoch 86/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 1243.8115 - val_loss: 2031.1668\n",
            "Epoch 87/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 1239.8333 - val_loss: 2016.4523\n",
            "Epoch 88/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 1222.2641 - val_loss: 1997.3115\n",
            "Epoch 89/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 1213.0279 - val_loss: 1978.1585\n",
            "Epoch 90/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 1202.3400 - val_loss: 1969.8663\n",
            "Epoch 91/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 1196.6489 - val_loss: 1955.9072\n",
            "Epoch 92/1100\n",
            "1291/1291 [==============================] - 0s 178us/step - loss: 1183.4982 - val_loss: 1940.1782\n",
            "Epoch 93/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 1170.5961 - val_loss: 1901.3571\n",
            "Epoch 94/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 1174.9351 - val_loss: 1888.1159\n",
            "Epoch 95/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 1154.5861 - val_loss: 1899.4496\n",
            "Epoch 96/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 1143.7955 - val_loss: 1889.5663\n",
            "Epoch 97/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 1134.1454 - val_loss: 1863.0624\n",
            "Epoch 98/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 1127.7555 - val_loss: 1858.3414\n",
            "Epoch 99/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 1120.2852 - val_loss: 1841.9416\n",
            "Epoch 100/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1108.1274 - val_loss: 1837.0108\n",
            "Epoch 101/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 1099.4269 - val_loss: 1824.6709\n",
            "Epoch 102/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 1095.5998 - val_loss: 1782.3318\n",
            "Epoch 103/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 1081.5780 - val_loss: 1772.8895\n",
            "Epoch 104/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 1067.0353 - val_loss: 1770.3560\n",
            "Epoch 105/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 1068.7066 - val_loss: 1738.4468\n",
            "Epoch 106/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 1064.8022 - val_loss: 1728.7675\n",
            "Epoch 107/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 1046.0165 - val_loss: 1753.4954\n",
            "Epoch 108/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 1031.9809 - val_loss: 1710.9565\n",
            "Epoch 109/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 1031.8036 - val_loss: 1717.1505\n",
            "Epoch 110/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 1022.4846 - val_loss: 1710.8042\n",
            "Epoch 111/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 1015.1352 - val_loss: 1744.4441\n",
            "Epoch 112/1100\n",
            "1291/1291 [==============================] - 0s 183us/step - loss: 1010.5917 - val_loss: 1692.1675\n",
            "Epoch 113/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 996.7262 - val_loss: 1670.3587\n",
            "Epoch 114/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 997.3925 - val_loss: 1682.7606\n",
            "Epoch 115/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 987.5742 - val_loss: 1636.0691\n",
            "Epoch 116/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 974.1039 - val_loss: 1683.2259\n",
            "Epoch 117/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 969.4129 - val_loss: 1620.2158\n",
            "Epoch 118/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 961.2373 - val_loss: 1622.0696\n",
            "Epoch 119/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 953.9766 - val_loss: 1588.2586\n",
            "Epoch 120/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 949.6855 - val_loss: 1613.4572\n",
            "Epoch 121/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 942.0655 - val_loss: 1606.8364\n",
            "Epoch 122/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 932.4729 - val_loss: 1574.5695\n",
            "Epoch 123/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 928.5908 - val_loss: 1580.9074\n",
            "Epoch 124/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 922.4381 - val_loss: 1572.8919\n",
            "Epoch 125/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 914.7312 - val_loss: 1544.0230\n",
            "Epoch 126/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 909.0572 - val_loss: 1574.8260\n",
            "Epoch 127/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 905.7331 - val_loss: 1521.4031\n",
            "Epoch 128/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 896.2732 - val_loss: 1518.3111\n",
            "Epoch 129/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 895.4141 - val_loss: 1509.6638\n",
            "Epoch 130/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 880.8070 - val_loss: 1512.7389\n",
            "Epoch 131/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 874.6822 - val_loss: 1485.3265\n",
            "Epoch 132/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 871.7338 - val_loss: 1480.1308\n",
            "Epoch 133/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 864.6722 - val_loss: 1453.0823\n",
            "Epoch 134/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 861.9999 - val_loss: 1441.5130\n",
            "Epoch 135/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 856.0897 - val_loss: 1460.2582\n",
            "Epoch 136/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 852.3730 - val_loss: 1421.6886\n",
            "Epoch 137/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 839.8185 - val_loss: 1462.9653\n",
            "Epoch 138/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 835.9848 - val_loss: 1419.5039\n",
            "Epoch 139/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 833.2585 - val_loss: 1410.0633\n",
            "Epoch 140/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 821.6765 - val_loss: 1420.2265\n",
            "Epoch 141/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 816.2314 - val_loss: 1378.3877\n",
            "Epoch 142/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 815.6469 - val_loss: 1397.3097\n",
            "Epoch 143/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 815.0390 - val_loss: 1383.5980\n",
            "Epoch 144/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 800.8232 - val_loss: 1409.9232\n",
            "Epoch 145/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 798.0603 - val_loss: 1371.8519\n",
            "Epoch 146/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 798.1070 - val_loss: 1362.6164\n",
            "Epoch 147/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 800.9939 - val_loss: 1365.5374\n",
            "Epoch 148/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 792.2526 - val_loss: 1356.4460\n",
            "Epoch 149/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 777.7461 - val_loss: 1317.0287\n",
            "Epoch 150/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 778.6626 - val_loss: 1346.6287\n",
            "Epoch 151/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 773.3666 - val_loss: 1334.2580\n",
            "Epoch 152/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 766.4960 - val_loss: 1326.1073\n",
            "Epoch 153/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 760.7606 - val_loss: 1310.0354\n",
            "Epoch 154/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 759.8655 - val_loss: 1347.8932\n",
            "Epoch 155/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 755.5737 - val_loss: 1309.8452\n",
            "Epoch 156/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 771.9212 - val_loss: 1295.2841\n",
            "Epoch 157/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 747.5040 - val_loss: 1273.0344\n",
            "Epoch 158/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 759.5983 - val_loss: 1298.9841\n",
            "Epoch 159/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 735.2266 - val_loss: 1273.5713\n",
            "Epoch 160/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 733.9206 - val_loss: 1239.2857\n",
            "Epoch 161/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 735.8846 - val_loss: 1254.8032\n",
            "Epoch 162/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 728.4905 - val_loss: 1250.5239\n",
            "Epoch 163/1100\n",
            "1291/1291 [==============================] - 0s 175us/step - loss: 724.0958 - val_loss: 1240.4978\n",
            "Epoch 164/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 723.8516 - val_loss: 1220.9607\n",
            "Epoch 165/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 717.1861 - val_loss: 1230.8232\n",
            "Epoch 166/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 716.9111 - val_loss: 1245.7486\n",
            "Epoch 167/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 713.8892 - val_loss: 1233.8438\n",
            "Epoch 168/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 707.8580 - val_loss: 1185.5437\n",
            "Epoch 169/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 703.1773 - val_loss: 1208.0650\n",
            "Epoch 170/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 701.4551 - val_loss: 1186.3714\n",
            "Epoch 171/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 695.5961 - val_loss: 1215.6389\n",
            "Epoch 172/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 691.4923 - val_loss: 1190.7698\n",
            "Epoch 173/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 700.2645 - val_loss: 1193.0955\n",
            "Epoch 174/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 688.2224 - val_loss: 1210.1931\n",
            "Epoch 175/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 701.2761 - val_loss: 1193.2342\n",
            "Epoch 176/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 686.8085 - val_loss: 1182.0667\n",
            "Epoch 177/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 676.0326 - val_loss: 1151.2326\n",
            "Epoch 178/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 673.0517 - val_loss: 1220.1503\n",
            "Epoch 179/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 681.2775 - val_loss: 1134.2757\n",
            "Epoch 180/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 676.3546 - val_loss: 1124.0714\n",
            "Epoch 181/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 672.9251 - val_loss: 1158.9120\n",
            "Epoch 182/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 665.6974 - val_loss: 1137.9779\n",
            "Epoch 183/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 659.5448 - val_loss: 1133.8877\n",
            "Epoch 184/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 665.6357 - val_loss: 1102.1047\n",
            "Epoch 185/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 660.7558 - val_loss: 1150.8166\n",
            "Epoch 186/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 653.3628 - val_loss: 1131.3976\n",
            "Epoch 187/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 652.2748 - val_loss: 1183.6256\n",
            "Epoch 188/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 658.5672 - val_loss: 1119.4250\n",
            "Epoch 189/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 649.4249 - val_loss: 1097.9880\n",
            "Epoch 190/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 639.8664 - val_loss: 1104.6564\n",
            "Epoch 191/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 645.5281 - val_loss: 1098.4859\n",
            "Epoch 192/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 637.2906 - val_loss: 1076.2627\n",
            "Epoch 193/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 645.4194 - val_loss: 1083.4161\n",
            "Epoch 194/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 639.2397 - val_loss: 1076.2350\n",
            "Epoch 195/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 633.1686 - val_loss: 1080.9060\n",
            "Epoch 196/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 636.0742 - val_loss: 1103.8485\n",
            "Epoch 197/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 624.0340 - val_loss: 1049.7763\n",
            "Epoch 198/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 622.0718 - val_loss: 1072.4946\n",
            "Epoch 199/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 622.1207 - val_loss: 1066.0383\n",
            "Epoch 200/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 621.3733 - val_loss: 1065.4372\n",
            "Epoch 201/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 619.3842 - val_loss: 1057.1797\n",
            "Epoch 202/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 614.5967 - val_loss: 1045.9867\n",
            "Epoch 203/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 613.6603 - val_loss: 1033.5653\n",
            "Epoch 204/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 614.7832 - val_loss: 1037.6323\n",
            "Epoch 205/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 608.7972 - val_loss: 1024.1451\n",
            "Epoch 206/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 611.5880 - val_loss: 1024.5478\n",
            "Epoch 207/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 608.2816 - val_loss: 1063.2778\n",
            "Epoch 208/1100\n",
            "1291/1291 [==============================] - 0s 167us/step - loss: 612.0454 - val_loss: 1026.4572\n",
            "Epoch 209/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 599.2622 - val_loss: 1020.4095\n",
            "Epoch 210/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 601.7101 - val_loss: 1012.7951\n",
            "Epoch 211/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 594.2306 - val_loss: 1023.2167\n",
            "Epoch 212/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 592.9669 - val_loss: 1011.5314\n",
            "Epoch 213/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 593.5502 - val_loss: 989.5342\n",
            "Epoch 214/1100\n",
            "1291/1291 [==============================] - 0s 181us/step - loss: 605.5953 - val_loss: 993.7982\n",
            "Epoch 215/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 599.9480 - val_loss: 1025.9652\n",
            "Epoch 216/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 587.3395 - val_loss: 993.4580\n",
            "Epoch 217/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 586.2348 - val_loss: 994.8074\n",
            "Epoch 218/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 587.2877 - val_loss: 1008.7745\n",
            "Epoch 219/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 585.0286 - val_loss: 994.4692\n",
            "Epoch 220/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 595.5492 - val_loss: 1014.3018\n",
            "Epoch 221/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 579.2696 - val_loss: 984.2354\n",
            "Epoch 222/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 580.8246 - val_loss: 996.3166\n",
            "Epoch 223/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 575.6476 - val_loss: 989.3026\n",
            "Epoch 224/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 582.7385 - val_loss: 972.8036\n",
            "Epoch 225/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 574.4317 - val_loss: 979.1154\n",
            "Epoch 226/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 571.2891 - val_loss: 967.6258\n",
            "Epoch 227/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 568.0731 - val_loss: 948.6494\n",
            "Epoch 228/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 569.7684 - val_loss: 983.9280\n",
            "Epoch 229/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 568.4537 - val_loss: 981.6191\n",
            "Epoch 230/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 571.5395 - val_loss: 970.5151\n",
            "Epoch 231/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 560.2308 - val_loss: 957.5385\n",
            "Epoch 232/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 560.3787 - val_loss: 947.4879\n",
            "Epoch 233/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 561.9322 - val_loss: 959.9977\n",
            "Epoch 234/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 561.6154 - val_loss: 925.8014\n",
            "Epoch 235/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 566.7265 - val_loss: 944.2556\n",
            "Epoch 236/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 554.2203 - val_loss: 943.3607\n",
            "Epoch 237/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 550.4288 - val_loss: 957.8189\n",
            "Epoch 238/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 551.4224 - val_loss: 918.4575\n",
            "Epoch 239/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 550.6258 - val_loss: 924.4884\n",
            "Epoch 240/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 548.6408 - val_loss: 919.4909\n",
            "Epoch 241/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 544.6490 - val_loss: 923.4016\n",
            "Epoch 242/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 543.1235 - val_loss: 927.5257\n",
            "Epoch 243/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 545.9203 - val_loss: 898.8730\n",
            "Epoch 244/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 541.2939 - val_loss: 922.5379\n",
            "Epoch 245/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 540.6466 - val_loss: 943.7871\n",
            "Epoch 246/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 541.3149 - val_loss: 904.0124\n",
            "Epoch 247/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 537.4239 - val_loss: 922.1843\n",
            "Epoch 248/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 541.9301 - val_loss: 893.2301\n",
            "Epoch 249/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 529.3669 - val_loss: 931.3971\n",
            "Epoch 250/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 534.1077 - val_loss: 905.2735\n",
            "Epoch 251/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 530.2778 - val_loss: 885.6499\n",
            "Epoch 252/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 532.6391 - val_loss: 910.3698\n",
            "Epoch 253/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 529.1400 - val_loss: 909.3234\n",
            "Epoch 254/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 528.6047 - val_loss: 902.1816\n",
            "Epoch 255/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 526.2980 - val_loss: 877.5884\n",
            "Epoch 256/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 528.3205 - val_loss: 881.2060\n",
            "Epoch 257/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 526.6575 - val_loss: 884.9740\n",
            "Epoch 258/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 520.4823 - val_loss: 878.4651\n",
            "Epoch 259/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 521.4096 - val_loss: 881.6124\n",
            "Epoch 260/1100\n",
            "1291/1291 [==============================] - 0s 170us/step - loss: 523.6602 - val_loss: 862.9837\n",
            "Epoch 261/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 517.5844 - val_loss: 905.4349\n",
            "Epoch 262/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 523.6170 - val_loss: 859.8627\n",
            "Epoch 263/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 520.2626 - val_loss: 861.1353\n",
            "Epoch 264/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 517.0093 - val_loss: 863.9575\n",
            "Epoch 265/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 514.6859 - val_loss: 888.4261\n",
            "Epoch 266/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 516.0423 - val_loss: 855.9463\n",
            "Epoch 267/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 517.0257 - val_loss: 867.2343\n",
            "Epoch 268/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 508.2252 - val_loss: 870.5020\n",
            "Epoch 269/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 508.3994 - val_loss: 859.1724\n",
            "Epoch 270/1100\n",
            "1291/1291 [==============================] - 0s 174us/step - loss: 509.3188 - val_loss: 855.2183\n",
            "Epoch 271/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 505.4780 - val_loss: 851.7776\n",
            "Epoch 272/1100\n",
            "1291/1291 [==============================] - 0s 171us/step - loss: 505.3288 - val_loss: 883.0020\n",
            "Epoch 273/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 512.3454 - val_loss: 848.5003\n",
            "Epoch 274/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 503.9245 - val_loss: 862.4424\n",
            "Epoch 275/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 502.3987 - val_loss: 849.8130\n",
            "Epoch 276/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 503.8796 - val_loss: 843.5689\n",
            "Epoch 277/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 500.8322 - val_loss: 838.4505\n",
            "Epoch 278/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 496.5658 - val_loss: 866.8134\n",
            "Epoch 279/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 502.9934 - val_loss: 884.8376\n",
            "Epoch 280/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 509.8177 - val_loss: 881.6123\n",
            "Epoch 281/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 496.3956 - val_loss: 842.9584\n",
            "Epoch 282/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 495.8115 - val_loss: 841.6161\n",
            "Epoch 283/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 501.0293 - val_loss: 834.9142\n",
            "Epoch 284/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 500.7316 - val_loss: 846.8403\n",
            "Epoch 285/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 494.0677 - val_loss: 835.8771\n",
            "Epoch 286/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 489.1186 - val_loss: 859.7385\n",
            "Epoch 287/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 489.8526 - val_loss: 837.3587\n",
            "Epoch 288/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 484.8897 - val_loss: 829.4132\n",
            "Epoch 289/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 488.2841 - val_loss: 822.0652\n",
            "Epoch 290/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 490.7030 - val_loss: 837.8229\n",
            "Epoch 291/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 491.4726 - val_loss: 840.1395\n",
            "Epoch 292/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 494.0054 - val_loss: 816.4132\n",
            "Epoch 293/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 485.3143 - val_loss: 815.4438\n",
            "Epoch 294/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 487.7705 - val_loss: 810.5973\n",
            "Epoch 295/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 481.2371 - val_loss: 820.6348\n",
            "Epoch 296/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 482.0681 - val_loss: 809.6088\n",
            "Epoch 297/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 476.5236 - val_loss: 829.0705\n",
            "Epoch 298/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 479.7766 - val_loss: 814.8366\n",
            "Epoch 299/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 475.1942 - val_loss: 823.4383\n",
            "Epoch 300/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 480.6456 - val_loss: 815.2909\n",
            "Epoch 301/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 477.5953 - val_loss: 807.7557\n",
            "Epoch 302/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 473.1858 - val_loss: 819.8649\n",
            "Epoch 303/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 489.1339 - val_loss: 827.1637\n",
            "Epoch 304/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 473.7398 - val_loss: 796.5660\n",
            "Epoch 305/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 478.0045 - val_loss: 834.6986\n",
            "Epoch 306/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 479.8379 - val_loss: 844.3781\n",
            "Epoch 307/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 483.4341 - val_loss: 822.9980\n",
            "Epoch 308/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 473.2696 - val_loss: 837.2461\n",
            "Epoch 309/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 472.3349 - val_loss: 817.5636\n",
            "Epoch 310/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 474.4768 - val_loss: 782.3264\n",
            "Epoch 311/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 470.4268 - val_loss: 790.5179\n",
            "Epoch 312/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 464.5849 - val_loss: 844.8143\n",
            "Epoch 313/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 471.3012 - val_loss: 803.4956\n",
            "Epoch 314/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 464.2557 - val_loss: 779.9233\n",
            "Epoch 315/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 465.3472 - val_loss: 783.2611\n",
            "Epoch 316/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 462.7973 - val_loss: 779.0830\n",
            "Epoch 317/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 462.7293 - val_loss: 806.4127\n",
            "Epoch 318/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 469.6488 - val_loss: 779.9093\n",
            "Epoch 319/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 461.7251 - val_loss: 803.1076\n",
            "Epoch 320/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 463.8082 - val_loss: 804.7720\n",
            "Epoch 321/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 458.9892 - val_loss: 771.7488\n",
            "Epoch 322/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 459.9895 - val_loss: 778.3870\n",
            "Epoch 323/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 457.7475 - val_loss: 770.2849\n",
            "Epoch 324/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 459.1361 - val_loss: 782.4789\n",
            "Epoch 325/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 461.9246 - val_loss: 791.1814\n",
            "Epoch 326/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 456.1744 - val_loss: 784.1362\n",
            "Epoch 327/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 456.0250 - val_loss: 780.7289\n",
            "Epoch 328/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 460.1140 - val_loss: 782.0946\n",
            "Epoch 329/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 454.5215 - val_loss: 778.8478\n",
            "Epoch 330/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 452.5542 - val_loss: 768.2975\n",
            "Epoch 331/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 451.6381 - val_loss: 763.0725\n",
            "Epoch 332/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 449.0477 - val_loss: 760.3635\n",
            "Epoch 333/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 451.0389 - val_loss: 757.6892\n",
            "Epoch 334/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 450.1222 - val_loss: 756.9389\n",
            "Epoch 335/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 457.0357 - val_loss: 761.9318\n",
            "Epoch 336/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 450.9261 - val_loss: 760.9803\n",
            "Epoch 337/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 459.6009 - val_loss: 753.9782\n",
            "Epoch 338/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 447.5829 - val_loss: 775.6318\n",
            "Epoch 339/1100\n",
            "1291/1291 [==============================] - 0s 174us/step - loss: 448.9715 - val_loss: 776.3690\n",
            "Epoch 340/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 447.2821 - val_loss: 769.4718\n",
            "Epoch 341/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 446.7416 - val_loss: 748.6052\n",
            "Epoch 342/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 444.6867 - val_loss: 758.9213\n",
            "Epoch 343/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 446.1856 - val_loss: 754.9541\n",
            "Epoch 344/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 440.9112 - val_loss: 752.9851\n",
            "Epoch 345/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 443.8802 - val_loss: 758.4330\n",
            "Epoch 346/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 440.8684 - val_loss: 755.4308\n",
            "Epoch 347/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 441.6439 - val_loss: 746.8457\n",
            "Epoch 348/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 442.3444 - val_loss: 751.3474\n",
            "Epoch 349/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 447.8058 - val_loss: 732.9498\n",
            "Epoch 350/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 438.5896 - val_loss: 775.7259\n",
            "Epoch 351/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 439.0667 - val_loss: 731.1517\n",
            "Epoch 352/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 436.5235 - val_loss: 745.8965\n",
            "Epoch 353/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 434.3000 - val_loss: 738.6834\n",
            "Epoch 354/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 440.4848 - val_loss: 759.4297\n",
            "Epoch 355/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 440.9512 - val_loss: 775.5763\n",
            "Epoch 356/1100\n",
            "1291/1291 [==============================] - 0s 167us/step - loss: 440.6927 - val_loss: 744.6960\n",
            "Epoch 357/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 437.1478 - val_loss: 735.7769\n",
            "Epoch 358/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 433.3979 - val_loss: 748.2843\n",
            "Epoch 359/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 432.7288 - val_loss: 733.7869\n",
            "Epoch 360/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 430.2427 - val_loss: 751.3948\n",
            "Epoch 361/1100\n",
            "1291/1291 [==============================] - 0s 170us/step - loss: 430.7977 - val_loss: 724.7238\n",
            "Epoch 362/1100\n",
            "1291/1291 [==============================] - 0s 140us/step - loss: 437.5488 - val_loss: 725.8991\n",
            "Epoch 363/1100\n",
            "1291/1291 [==============================] - 0s 139us/step - loss: 430.5419 - val_loss: 726.3350\n",
            "Epoch 364/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 431.0948 - val_loss: 737.2152\n",
            "Epoch 365/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 435.0977 - val_loss: 727.7224\n",
            "Epoch 366/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 435.0990 - val_loss: 736.2743\n",
            "Epoch 367/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 435.2199 - val_loss: 723.4671\n",
            "Epoch 368/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 425.8378 - val_loss: 725.1314\n",
            "Epoch 369/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 426.7745 - val_loss: 728.8788\n",
            "Epoch 370/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 425.6366 - val_loss: 721.6587\n",
            "Epoch 371/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 425.8559 - val_loss: 717.0826\n",
            "Epoch 372/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 423.2975 - val_loss: 750.1025\n",
            "Epoch 373/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 421.6285 - val_loss: 712.0173\n",
            "Epoch 374/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 428.3220 - val_loss: 710.2586\n",
            "Epoch 375/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 426.0576 - val_loss: 703.5440\n",
            "Epoch 376/1100\n",
            "1291/1291 [==============================] - 0s 175us/step - loss: 419.2970 - val_loss: 713.5256\n",
            "Epoch 377/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 418.2083 - val_loss: 699.7522\n",
            "Epoch 378/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 417.3222 - val_loss: 706.6005\n",
            "Epoch 379/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 417.5545 - val_loss: 727.3162\n",
            "Epoch 380/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 423.6728 - val_loss: 762.8388\n",
            "Epoch 381/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 430.2963 - val_loss: 705.7154\n",
            "Epoch 382/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 419.2229 - val_loss: 714.5330\n",
            "Epoch 383/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 412.8017 - val_loss: 696.4660\n",
            "Epoch 384/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 417.4062 - val_loss: 698.3176\n",
            "Epoch 385/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 423.2965 - val_loss: 714.2074\n",
            "Epoch 386/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 412.6861 - val_loss: 689.5786\n",
            "Epoch 387/1100\n",
            "1291/1291 [==============================] - 0s 173us/step - loss: 414.3580 - val_loss: 699.8582\n",
            "Epoch 388/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 410.1960 - val_loss: 705.7552\n",
            "Epoch 389/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 416.9407 - val_loss: 688.2053\n",
            "Epoch 390/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 410.8710 - val_loss: 697.9995\n",
            "Epoch 391/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 408.2469 - val_loss: 687.2535\n",
            "Epoch 392/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 425.1960 - val_loss: 687.5131\n",
            "Epoch 393/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 416.0549 - val_loss: 696.0498\n",
            "Epoch 394/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 413.4582 - val_loss: 689.5958\n",
            "Epoch 395/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 410.3527 - val_loss: 686.3173\n",
            "Epoch 396/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 408.7555 - val_loss: 681.2894\n",
            "Epoch 397/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 407.5842 - val_loss: 697.0251\n",
            "Epoch 398/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 409.8539 - val_loss: 714.4432\n",
            "Epoch 399/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 415.6431 - val_loss: 697.8884\n",
            "Epoch 400/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 407.8134 - val_loss: 703.9010\n",
            "Epoch 401/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 404.1667 - val_loss: 672.9792\n",
            "Epoch 402/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 411.3450 - val_loss: 686.3550\n",
            "Epoch 403/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 405.3445 - val_loss: 684.2637\n",
            "Epoch 404/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 402.4546 - val_loss: 702.6814\n",
            "Epoch 405/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 407.3645 - val_loss: 711.9293\n",
            "Epoch 406/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 411.7654 - val_loss: 677.4029\n",
            "Epoch 407/1100\n",
            "1291/1291 [==============================] - 0s 171us/step - loss: 400.4037 - val_loss: 678.4323\n",
            "Epoch 408/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 401.8100 - val_loss: 671.1543\n",
            "Epoch 409/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 398.2722 - val_loss: 665.4195\n",
            "Epoch 410/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 400.4785 - val_loss: 669.7108\n",
            "Epoch 411/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 401.1093 - val_loss: 667.1016\n",
            "Epoch 412/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 398.2863 - val_loss: 693.5016\n",
            "Epoch 413/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 402.3737 - val_loss: 664.2004\n",
            "Epoch 414/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 396.2169 - val_loss: 673.6722\n",
            "Epoch 415/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 393.3772 - val_loss: 657.8423\n",
            "Epoch 416/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 399.2955 - val_loss: 678.1954\n",
            "Epoch 417/1100\n",
            "1291/1291 [==============================] - 0s 168us/step - loss: 394.1905 - val_loss: 663.2823\n",
            "Epoch 418/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 393.4027 - val_loss: 663.4463\n",
            "Epoch 419/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 394.7768 - val_loss: 656.8558\n",
            "Epoch 420/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 394.9112 - val_loss: 652.3631\n",
            "Epoch 421/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 397.1899 - val_loss: 656.0952\n",
            "Epoch 422/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 392.3049 - val_loss: 661.8560\n",
            "Epoch 423/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 391.6299 - val_loss: 647.0883\n",
            "Epoch 424/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 391.2631 - val_loss: 651.0613\n",
            "Epoch 425/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 392.9332 - val_loss: 652.1404\n",
            "Epoch 426/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 389.5366 - val_loss: 646.6513\n",
            "Epoch 427/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 389.8106 - val_loss: 659.6429\n",
            "Epoch 428/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 386.2448 - val_loss: 660.7541\n",
            "Epoch 429/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 391.1653 - val_loss: 652.6189\n",
            "Epoch 430/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 387.1570 - val_loss: 656.0434\n",
            "Epoch 431/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 389.2186 - val_loss: 642.0979\n",
            "Epoch 432/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 387.5712 - val_loss: 643.6712\n",
            "Epoch 433/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 388.8120 - val_loss: 642.2636\n",
            "Epoch 434/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 387.0074 - val_loss: 646.4452\n",
            "Epoch 435/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 390.1399 - val_loss: 643.1557\n",
            "Epoch 436/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 385.7183 - val_loss: 639.3338\n",
            "Epoch 437/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 383.2612 - val_loss: 663.6072\n",
            "Epoch 438/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 386.8355 - val_loss: 638.4320\n",
            "Epoch 439/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 379.4552 - val_loss: 640.4391\n",
            "Epoch 440/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 380.0386 - val_loss: 628.1034\n",
            "Epoch 441/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 377.6746 - val_loss: 633.7384\n",
            "Epoch 442/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 380.5335 - val_loss: 637.5467\n",
            "Epoch 443/1100\n",
            "1291/1291 [==============================] - 0s 167us/step - loss: 380.2520 - val_loss: 634.4226\n",
            "Epoch 444/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 381.6781 - val_loss: 622.7431\n",
            "Epoch 445/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 379.9039 - val_loss: 625.0973\n",
            "Epoch 446/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 379.3132 - val_loss: 637.0214\n",
            "Epoch 447/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 377.0131 - val_loss: 626.9647\n",
            "Epoch 448/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 374.8089 - val_loss: 624.2516\n",
            "Epoch 449/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 376.1832 - val_loss: 630.0338\n",
            "Epoch 450/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 376.3629 - val_loss: 628.7823\n",
            "Epoch 451/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 373.2517 - val_loss: 617.2264\n",
            "Epoch 452/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 373.2761 - val_loss: 619.1341\n",
            "Epoch 453/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 373.1025 - val_loss: 622.5642\n",
            "Epoch 454/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 378.9171 - val_loss: 616.5997\n",
            "Epoch 455/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 375.3143 - val_loss: 634.6082\n",
            "Epoch 456/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 373.9369 - val_loss: 615.1350\n",
            "Epoch 457/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 374.9331 - val_loss: 619.5077\n",
            "Epoch 458/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 370.5765 - val_loss: 618.2019\n",
            "Epoch 459/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 369.8517 - val_loss: 623.7975\n",
            "Epoch 460/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 371.1364 - val_loss: 620.4679\n",
            "Epoch 461/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 367.9882 - val_loss: 611.8075\n",
            "Epoch 462/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 367.3327 - val_loss: 617.3091\n",
            "Epoch 463/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 365.4054 - val_loss: 626.5195\n",
            "Epoch 464/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 366.4462 - val_loss: 633.1345\n",
            "Epoch 465/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 368.8420 - val_loss: 633.2792\n",
            "Epoch 466/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 364.6096 - val_loss: 620.1072\n",
            "Epoch 467/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 364.9831 - val_loss: 613.2686\n",
            "Epoch 468/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 365.7922 - val_loss: 610.7016\n",
            "Epoch 469/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 364.5605 - val_loss: 620.1948\n",
            "Epoch 470/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 363.8578 - val_loss: 601.7180\n",
            "Epoch 471/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 362.2052 - val_loss: 611.4256\n",
            "Epoch 472/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 368.0042 - val_loss: 623.6563\n",
            "Epoch 473/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 365.4778 - val_loss: 605.9068\n",
            "Epoch 474/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 360.3267 - val_loss: 601.3671\n",
            "Epoch 475/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 364.3771 - val_loss: 599.6344\n",
            "Epoch 476/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 361.9742 - val_loss: 607.9194\n",
            "Epoch 477/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 360.3393 - val_loss: 612.2633\n",
            "Epoch 478/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 360.2092 - val_loss: 607.0552\n",
            "Epoch 479/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 371.5559 - val_loss: 609.2504\n",
            "Epoch 480/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 360.2138 - val_loss: 622.0022\n",
            "Epoch 481/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 358.6180 - val_loss: 602.2520\n",
            "Epoch 482/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 355.4323 - val_loss: 615.0860\n",
            "Epoch 483/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 362.5578 - val_loss: 604.4796\n",
            "Epoch 484/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 365.7768 - val_loss: 597.0092\n",
            "Epoch 485/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 356.2315 - val_loss: 605.6009\n",
            "Epoch 486/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 361.8621 - val_loss: 603.1024\n",
            "Epoch 487/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 359.0708 - val_loss: 596.0963\n",
            "Epoch 488/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 359.6687 - val_loss: 601.8303\n",
            "Epoch 489/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 355.5928 - val_loss: 599.9954\n",
            "Epoch 490/1100\n",
            "1291/1291 [==============================] - 0s 168us/step - loss: 355.5807 - val_loss: 596.0465\n",
            "Epoch 491/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 352.8278 - val_loss: 597.9411\n",
            "Epoch 492/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 352.2318 - val_loss: 595.6055\n",
            "Epoch 493/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 352.4453 - val_loss: 613.9929\n",
            "Epoch 494/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 351.0532 - val_loss: 597.0168\n",
            "Epoch 495/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 351.3009 - val_loss: 602.7935\n",
            "Epoch 496/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 350.8437 - val_loss: 598.0480\n",
            "Epoch 497/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 352.4767 - val_loss: 611.5332\n",
            "Epoch 498/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 350.7397 - val_loss: 588.8616\n",
            "Epoch 499/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 355.4156 - val_loss: 593.3206\n",
            "Epoch 500/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 350.9634 - val_loss: 599.4049\n",
            "Epoch 501/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 345.8016 - val_loss: 589.7565\n",
            "Epoch 502/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 355.0023 - val_loss: 587.3346\n",
            "Epoch 503/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 346.5537 - val_loss: 596.0844\n",
            "Epoch 504/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 348.0479 - val_loss: 597.3322\n",
            "Epoch 505/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 348.6593 - val_loss: 590.5471\n",
            "Epoch 506/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 348.2692 - val_loss: 600.1886\n",
            "Epoch 507/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 346.9770 - val_loss: 600.8283\n",
            "Epoch 508/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 345.2790 - val_loss: 596.9428\n",
            "Epoch 509/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 341.0069 - val_loss: 608.3557\n",
            "Epoch 510/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 342.5877 - val_loss: 586.6655\n",
            "Epoch 511/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 343.1645 - val_loss: 584.4618\n",
            "Epoch 512/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 343.9755 - val_loss: 595.2465\n",
            "Epoch 513/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 342.2751 - val_loss: 585.5240\n",
            "Epoch 514/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 342.2465 - val_loss: 590.6971\n",
            "Epoch 515/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 345.5924 - val_loss: 592.0149\n",
            "Epoch 516/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 340.6158 - val_loss: 585.5719\n",
            "Epoch 517/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 342.6184 - val_loss: 578.0648\n",
            "Epoch 518/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 339.5820 - val_loss: 577.6866\n",
            "Epoch 519/1100\n",
            "1291/1291 [==============================] - 0s 166us/step - loss: 337.5466 - val_loss: 577.0148\n",
            "Epoch 520/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 338.7201 - val_loss: 580.1271\n",
            "Epoch 521/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 336.7771 - val_loss: 586.0856\n",
            "Epoch 522/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 335.6574 - val_loss: 575.8669\n",
            "Epoch 523/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 347.0161 - val_loss: 576.6011\n",
            "Epoch 524/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 334.8505 - val_loss: 580.4979\n",
            "Epoch 525/1100\n",
            "1291/1291 [==============================] - 0s 169us/step - loss: 336.4330 - val_loss: 587.9807\n",
            "Epoch 526/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 336.5252 - val_loss: 570.0066\n",
            "Epoch 527/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 332.6492 - val_loss: 588.5762\n",
            "Epoch 528/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 337.6305 - val_loss: 572.3839\n",
            "Epoch 529/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 333.7164 - val_loss: 602.1880\n",
            "Epoch 530/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 333.9904 - val_loss: 567.9143\n",
            "Epoch 531/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 331.1710 - val_loss: 578.6382\n",
            "Epoch 532/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 332.2681 - val_loss: 576.6816\n",
            "Epoch 533/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 331.7534 - val_loss: 587.0780\n",
            "Epoch 534/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 334.1026 - val_loss: 574.4752\n",
            "Epoch 535/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 336.1279 - val_loss: 566.4436\n",
            "Epoch 536/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 327.6735 - val_loss: 578.8153\n",
            "Epoch 537/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 336.8778 - val_loss: 577.5390\n",
            "Epoch 538/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 332.8226 - val_loss: 584.9734\n",
            "Epoch 539/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 330.0472 - val_loss: 565.2643\n",
            "Epoch 540/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 331.5594 - val_loss: 576.4988\n",
            "Epoch 541/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 327.6055 - val_loss: 566.5873\n",
            "Epoch 542/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 332.5602 - val_loss: 564.5799\n",
            "Epoch 543/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 328.7122 - val_loss: 579.0676\n",
            "Epoch 544/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 330.4583 - val_loss: 561.4889\n",
            "Epoch 545/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 326.8182 - val_loss: 560.1856\n",
            "Epoch 546/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 325.6489 - val_loss: 569.2049\n",
            "Epoch 547/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 325.5925 - val_loss: 559.1544\n",
            "Epoch 548/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 327.9211 - val_loss: 565.9608\n",
            "Epoch 549/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 325.0613 - val_loss: 559.7070\n",
            "Epoch 550/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 328.1159 - val_loss: 561.4616\n",
            "Epoch 551/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 325.8022 - val_loss: 558.8976\n",
            "Epoch 552/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 329.5858 - val_loss: 556.1250\n",
            "Epoch 553/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 321.3102 - val_loss: 568.6165\n",
            "Epoch 554/1100\n",
            "1291/1291 [==============================] - 0s 147us/step - loss: 330.3642 - val_loss: 579.0217\n",
            "Epoch 555/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 320.2097 - val_loss: 561.5448\n",
            "Epoch 556/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 324.6952 - val_loss: 564.0969\n",
            "Epoch 557/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 335.8141 - val_loss: 564.5977\n",
            "Epoch 558/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 326.2208 - val_loss: 566.7238\n",
            "Epoch 559/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 320.1243 - val_loss: 553.4424\n",
            "Epoch 560/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 319.3934 - val_loss: 557.8660\n",
            "Epoch 561/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 318.4304 - val_loss: 550.2997\n",
            "Epoch 562/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 322.8315 - val_loss: 569.0207\n",
            "Epoch 563/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 321.3940 - val_loss: 563.4967\n",
            "Epoch 564/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 318.3425 - val_loss: 568.1758\n",
            "Epoch 565/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 318.6963 - val_loss: 552.3135\n",
            "Epoch 566/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 317.6700 - val_loss: 551.2088\n",
            "Epoch 567/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 319.4739 - val_loss: 551.6805\n",
            "Epoch 568/1100\n",
            "1291/1291 [==============================] - 0s 155us/step - loss: 315.8541 - val_loss: 558.0683\n",
            "Epoch 569/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 318.5522 - val_loss: 562.8230\n",
            "Epoch 570/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 319.4622 - val_loss: 544.3208\n",
            "Epoch 571/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 317.1291 - val_loss: 545.4150\n",
            "Epoch 572/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 313.2240 - val_loss: 555.3989\n",
            "Epoch 573/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 316.8478 - val_loss: 548.3979\n",
            "Epoch 574/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 313.8157 - val_loss: 547.6461\n",
            "Epoch 575/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 311.8153 - val_loss: 541.6213\n",
            "Epoch 576/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 317.4025 - val_loss: 546.1892\n",
            "Epoch 577/1100\n",
            "1291/1291 [==============================] - 0s 141us/step - loss: 313.8903 - val_loss: 544.0189\n",
            "Epoch 578/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 313.4512 - val_loss: 551.3305\n",
            "Epoch 579/1100\n",
            "1291/1291 [==============================] - 0s 140us/step - loss: 310.4019 - val_loss: 539.4913\n",
            "Epoch 580/1100\n",
            "1291/1291 [==============================] - 0s 151us/step - loss: 316.0399 - val_loss: 537.8308\n",
            "Epoch 581/1100\n",
            "1291/1291 [==============================] - 0s 165us/step - loss: 316.7762 - val_loss: 539.5204\n",
            "Epoch 582/1100\n",
            "1291/1291 [==============================] - 0s 143us/step - loss: 309.9239 - val_loss: 541.6042\n",
            "Epoch 583/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 311.3235 - val_loss: 546.0615\n",
            "Epoch 584/1100\n",
            "1291/1291 [==============================] - 0s 164us/step - loss: 307.4431 - val_loss: 536.7608\n",
            "Epoch 585/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 312.3340 - val_loss: 552.2450\n",
            "Epoch 586/1100\n",
            "1291/1291 [==============================] - 0s 161us/step - loss: 310.9337 - val_loss: 541.8899\n",
            "Epoch 587/1100\n",
            "1291/1291 [==============================] - 0s 149us/step - loss: 308.5317 - val_loss: 535.5132\n",
            "Epoch 588/1100\n",
            "1291/1291 [==============================] - 0s 159us/step - loss: 305.6087 - val_loss: 545.7655\n",
            "Epoch 589/1100\n",
            "1291/1291 [==============================] - 0s 157us/step - loss: 307.9565 - val_loss: 542.5320\n",
            "Epoch 590/1100\n",
            "1291/1291 [==============================] - 0s 154us/step - loss: 307.5398 - val_loss: 537.1149\n",
            "Epoch 591/1100\n",
            "1291/1291 [==============================] - 0s 173us/step - loss: 307.1906 - val_loss: 532.5776\n",
            "Epoch 592/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 306.9711 - val_loss: 536.4176\n",
            "Epoch 593/1100\n",
            "1291/1291 [==============================] - 0s 150us/step - loss: 305.4422 - val_loss: 532.6241\n",
            "Epoch 594/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 309.2530 - val_loss: 540.3367\n",
            "Epoch 595/1100\n",
            "1291/1291 [==============================] - 0s 148us/step - loss: 305.2391 - val_loss: 533.4648\n",
            "Epoch 596/1100\n",
            "1291/1291 [==============================] - 0s 163us/step - loss: 304.1959 - val_loss: 542.1901\n",
            "Epoch 597/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 308.8933 - val_loss: 539.9476\n",
            "Epoch 598/1100\n",
            "1291/1291 [==============================] - 0s 158us/step - loss: 306.6448 - val_loss: 531.5184\n",
            "Epoch 599/1100\n",
            "1291/1291 [==============================] - 0s 168us/step - loss: 303.2266 - val_loss: 529.7076\n",
            "Epoch 600/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 305.8619 - val_loss: 530.2998\n",
            "Epoch 601/1100\n",
            "1291/1291 [==============================] - 0s 156us/step - loss: 303.2851 - val_loss: 534.0039\n",
            "Epoch 602/1100\n",
            "1291/1291 [==============================] - 0s 162us/step - loss: 303.5368 - val_loss: 540.1374\n",
            "Epoch 603/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 301.7565 - val_loss: 530.0285\n",
            "Epoch 604/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 300.5270 - val_loss: 541.3590\n",
            "Epoch 605/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 303.4911 - val_loss: 527.6413\n",
            "Epoch 606/1100\n",
            "1291/1291 [==============================] - 0s 153us/step - loss: 299.6243 - val_loss: 525.2831\n",
            "Epoch 607/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 298.8541 - val_loss: 538.5755\n",
            "Epoch 608/1100\n",
            "1291/1291 [==============================] - 0s 146us/step - loss: 300.2141 - val_loss: 524.8945\n",
            "Epoch 609/1100\n",
            "1291/1291 [==============================] - 0s 140us/step - loss: 303.5614 - val_loss: 522.5198\n",
            "Epoch 610/1100\n",
            "1291/1291 [==============================] - 0s 142us/step - loss: 298.1606 - val_loss: 532.1005\n",
            "Epoch 611/1100\n",
            "1291/1291 [==============================] - 0s 145us/step - loss: 304.7120 - val_loss: 525.8859\n",
            "Epoch 612/1100\n",
            "1291/1291 [==============================] - 0s 160us/step - loss: 299.3382 - val_loss: 533.7080\n",
            "Epoch 613/1100\n",
            "1291/1291 [==============================] - 0s 138us/step - loss: 301.6433 - val_loss: 523.6723\n",
            "Epoch 614/1100\n",
            "1291/1291 [==============================] - 0s 152us/step - loss: 296.0609 - val_loss: 525.9950\n",
            "Epoch 615/1100\n",
            "1291/1291 [==============================] - 0s 144us/step - loss: 295.7181 - val_loss: 526.5861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcj9WCv8gfWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Plotting the learning curve \n",
        "\n",
        "tr = hist.history['loss']\n",
        "val_ls = hist.history['val_loss']\n",
        "plt.plot(tr[1:])\n",
        "plt.plot(val_ls[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gryPzko7gjTS",
        "colab_type": "code",
        "outputId": "9f26b576-e37b-41ef-f1df-3b439d9a0f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Estimating the Root Mean Square Error of the Model - A metric of its performance\n",
        "\n",
        "predicted = model.predict([x_test_t])\n",
        "y_test_t=np.reshape(y_test_t, (np.shape(y_test_t)[0],1))\n",
        "\n",
        "actual =[]\n",
        "pred=[]\n",
        "se=0\n",
        "show = np.hstack((y_test_t, predicted))\n",
        "\n",
        "for i in show:\n",
        "  se+=(i[0]-i[1])**2\n",
        "  actual.append(i[0])\n",
        "  pred.append(i[1])\n",
        "  \n",
        "rmse = (se/np.size(y_test_t))**0.5\n",
        "print(rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f54aafd9d319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZFQGvBiOxg",
        "colab_type": "code",
        "outputId": "70d380fa-98a3-447f-c644-deb52bae853f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.plot(actual)\n",
        "plt.plot(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f10c029b2e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peDzBdcqkjzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}