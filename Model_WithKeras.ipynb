{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_WithKeras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/Celestini/blob/master/Model_WithKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB_ovQ-XfhRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "490b73d6-6b64-4dae-94e6-693eb1370044"
      },
      "source": [
        "import tensorflow as tf \n",
        "import pickle\n",
        "from sklearn.externals import joblib\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.saved_model import simple_save\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python import pywrap_tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqfmC0FpnwAX",
        "colab_type": "text"
      },
      "source": [
        "The labels and Features files for the model have been listed in the repository to be included here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPlZWUSf04p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "with open('/content/labelsFinal_withprevpm.bin','rb') as file:\n",
        "  labels = pickle.load(file)\n",
        "  \n",
        "with open('/content/featuresFinal_withprevpm.bin','rb') as file:\n",
        "  features = pickle.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWX04Xrf5pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_t, x_test_t,y_train_t, y_test_t = train_test_split(features, labels, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKhLYx25gDmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBsJRRwLgUtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "9f6a2fc2-a05f-41a9-e885-06110aa4c240"
      },
      "source": [
        "print(device_lib.list_local_devices())\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17128103730301002164\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 1388153605367717769\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2538875807017497611\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11189410202\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 1101547174280932497\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U5s3T6kgWmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7f458806-31e7-4c4c-fc70-737a855f528b"
      },
      "source": [
        "\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "model.add(Dense(15, input_shape = (10, ), activation = 'relu'))\n",
        "model.add(Dense(25, activation = 'relu'))\n",
        "model.add(Dense(1,  activation = 'relu'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 17:45:09.633952 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0830 17:45:09.638928 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0830 17:45:09.642008 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy1vyNq3gZGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "2359fb49-06bc-47a8-f78f-93606d61fd4f"
      },
      "source": [
        "\n",
        "model.compile(optimizer = Adam(lr = 0.001),loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 17:45:09.716118 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 15)                165       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                400       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 26        \n",
            "=================================================================\n",
            "Total params: 591\n",
            "Trainable params: 591\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTkDmTcgbdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcd468f7-b137-4153-d21c-5b2ef352d2e9"
      },
      "source": [
        "hist = model.fit([x_train_t], [y_train_t], validation_split = 0.33, epochs = 1400,verbose = 1, batch_size = 32, shuffle = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 17:45:11.721405 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0830 17:45:11.820966 139711710766976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1158 samples, validate on 571 samples\n",
            "Epoch 1/1400\n",
            "1158/1158 [==============================] - 1s 907us/step - loss: 3487.4776 - val_loss: 3155.5235\n",
            "Epoch 2/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 3272.5535 - val_loss: 2867.7414\n",
            "Epoch 3/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 2866.0698 - val_loss: 2321.4404\n",
            "Epoch 4/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 2175.6654 - val_loss: 1493.5687\n",
            "Epoch 5/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 1356.3473 - val_loss: 815.6367\n",
            "Epoch 6/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 916.1756 - val_loss: 644.4339\n",
            "Epoch 7/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 842.3950 - val_loss: 630.6178\n",
            "Epoch 8/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 827.3140 - val_loss: 621.1492\n",
            "Epoch 9/1400\n",
            "1158/1158 [==============================] - 0s 212us/step - loss: 814.8474 - val_loss: 610.9870\n",
            "Epoch 10/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 800.6168 - val_loss: 600.7691\n",
            "Epoch 11/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 786.8855 - val_loss: 589.2871\n",
            "Epoch 12/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 772.4973 - val_loss: 577.2150\n",
            "Epoch 13/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 755.3628 - val_loss: 564.5880\n",
            "Epoch 14/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 737.9722 - val_loss: 551.3454\n",
            "Epoch 15/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 720.9878 - val_loss: 539.3714\n",
            "Epoch 16/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 704.4148 - val_loss: 527.1364\n",
            "Epoch 17/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 687.0440 - val_loss: 515.4188\n",
            "Epoch 18/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 670.6056 - val_loss: 503.4599\n",
            "Epoch 19/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 655.3307 - val_loss: 492.4940\n",
            "Epoch 20/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 638.4864 - val_loss: 481.5374\n",
            "Epoch 21/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 622.4910 - val_loss: 470.4915\n",
            "Epoch 22/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 606.6639 - val_loss: 460.7821\n",
            "Epoch 23/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 590.9320 - val_loss: 450.0703\n",
            "Epoch 24/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 573.8536 - val_loss: 439.3957\n",
            "Epoch 25/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 558.6971 - val_loss: 429.9859\n",
            "Epoch 26/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 544.6890 - val_loss: 420.4228\n",
            "Epoch 27/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 529.6211 - val_loss: 410.8804\n",
            "Epoch 28/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 513.5727 - val_loss: 401.5006\n",
            "Epoch 29/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 498.7023 - val_loss: 393.2252\n",
            "Epoch 30/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 484.9571 - val_loss: 383.0624\n",
            "Epoch 31/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 470.3919 - val_loss: 374.7525\n",
            "Epoch 32/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 457.4331 - val_loss: 366.6828\n",
            "Epoch 33/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 444.0677 - val_loss: 359.0280\n",
            "Epoch 34/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 431.4887 - val_loss: 349.8190\n",
            "Epoch 35/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 417.7542 - val_loss: 342.2057\n",
            "Epoch 36/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 406.3409 - val_loss: 333.7389\n",
            "Epoch 37/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 393.9229 - val_loss: 326.1964\n",
            "Epoch 38/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 383.2864 - val_loss: 316.6980\n",
            "Epoch 39/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 372.8650 - val_loss: 309.1274\n",
            "Epoch 40/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 361.0410 - val_loss: 302.4354\n",
            "Epoch 41/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 351.2453 - val_loss: 294.2051\n",
            "Epoch 42/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 341.0179 - val_loss: 286.5088\n",
            "Epoch 43/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 329.4831 - val_loss: 279.5385\n",
            "Epoch 44/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 320.8439 - val_loss: 271.7519\n",
            "Epoch 45/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 312.3002 - val_loss: 265.2669\n",
            "Epoch 46/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 306.0721 - val_loss: 257.7961\n",
            "Epoch 47/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 296.5033 - val_loss: 250.0060\n",
            "Epoch 48/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 288.9251 - val_loss: 244.2774\n",
            "Epoch 49/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 281.6564 - val_loss: 238.5335\n",
            "Epoch 50/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 275.6231 - val_loss: 233.2052\n",
            "Epoch 51/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 268.9106 - val_loss: 228.0716\n",
            "Epoch 52/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 264.5474 - val_loss: 222.6467\n",
            "Epoch 53/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 258.8091 - val_loss: 219.0027\n",
            "Epoch 54/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 255.1678 - val_loss: 215.3280\n",
            "Epoch 55/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 250.1788 - val_loss: 211.8480\n",
            "Epoch 56/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 244.8615 - val_loss: 209.1085\n",
            "Epoch 57/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 241.2979 - val_loss: 205.1812\n",
            "Epoch 58/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 237.5884 - val_loss: 202.0246\n",
            "Epoch 59/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 236.2709 - val_loss: 199.7264\n",
            "Epoch 60/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 232.8501 - val_loss: 196.8759\n",
            "Epoch 61/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 229.7050 - val_loss: 194.6919\n",
            "Epoch 62/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 229.3466 - val_loss: 193.6126\n",
            "Epoch 63/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 229.3292 - val_loss: 191.8231\n",
            "Epoch 64/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 225.3998 - val_loss: 190.0673\n",
            "Epoch 65/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 223.8127 - val_loss: 189.2971\n",
            "Epoch 66/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 223.9505 - val_loss: 188.4613\n",
            "Epoch 67/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 222.1250 - val_loss: 187.5532\n",
            "Epoch 68/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 221.7798 - val_loss: 186.4514\n",
            "Epoch 69/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 220.4027 - val_loss: 186.2492\n",
            "Epoch 70/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 221.8618 - val_loss: 185.5119\n",
            "Epoch 71/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 219.9372 - val_loss: 185.5174\n",
            "Epoch 72/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 219.5979 - val_loss: 184.2879\n",
            "Epoch 73/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 218.9978 - val_loss: 183.3901\n",
            "Epoch 74/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 218.6418 - val_loss: 184.3702\n",
            "Epoch 75/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 218.2953 - val_loss: 182.3761\n",
            "Epoch 76/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 216.4770 - val_loss: 182.7993\n",
            "Epoch 77/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 216.0449 - val_loss: 182.0015\n",
            "Epoch 78/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 217.9292 - val_loss: 181.5108\n",
            "Epoch 79/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 217.3681 - val_loss: 181.6461\n",
            "Epoch 80/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 215.1270 - val_loss: 182.6524\n",
            "Epoch 81/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 215.8630 - val_loss: 180.9520\n",
            "Epoch 82/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 215.1812 - val_loss: 180.4758\n",
            "Epoch 83/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 214.4778 - val_loss: 179.9253\n",
            "Epoch 84/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 213.9091 - val_loss: 179.8613\n",
            "Epoch 85/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 213.9647 - val_loss: 179.8184\n",
            "Epoch 86/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 213.4874 - val_loss: 180.6999\n",
            "Epoch 87/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 213.8540 - val_loss: 178.9906\n",
            "Epoch 88/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 213.2108 - val_loss: 178.4112\n",
            "Epoch 89/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 212.7569 - val_loss: 179.3021\n",
            "Epoch 90/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 212.3814 - val_loss: 179.0633\n",
            "Epoch 91/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 215.3000 - val_loss: 184.7963\n",
            "Epoch 92/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 216.2195 - val_loss: 178.9670\n",
            "Epoch 93/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 211.5553 - val_loss: 178.0849\n",
            "Epoch 94/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 211.6344 - val_loss: 177.7778\n",
            "Epoch 95/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 211.2545 - val_loss: 176.9500\n",
            "Epoch 96/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 210.6543 - val_loss: 178.0313\n",
            "Epoch 97/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 210.9824 - val_loss: 177.5724\n",
            "Epoch 98/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 210.5464 - val_loss: 177.1390\n",
            "Epoch 99/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 209.7337 - val_loss: 176.2823\n",
            "Epoch 100/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 209.5018 - val_loss: 176.4087\n",
            "Epoch 101/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 209.6974 - val_loss: 176.0063\n",
            "Epoch 102/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 210.4365 - val_loss: 175.9723\n",
            "Epoch 103/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 209.2612 - val_loss: 175.4176\n",
            "Epoch 104/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 213.1776 - val_loss: 175.8497\n",
            "Epoch 105/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 207.4967 - val_loss: 175.6608\n",
            "Epoch 106/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 208.7481 - val_loss: 175.4309\n",
            "Epoch 107/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 207.7782 - val_loss: 175.6518\n",
            "Epoch 108/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 208.5621 - val_loss: 174.0898\n",
            "Epoch 109/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 208.4685 - val_loss: 175.0544\n",
            "Epoch 110/1400\n",
            "1158/1158 [==============================] - 0s 207us/step - loss: 207.5947 - val_loss: 173.5020\n",
            "Epoch 111/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 207.6999 - val_loss: 173.2910\n",
            "Epoch 112/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 207.3302 - val_loss: 173.0876\n",
            "Epoch 113/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 206.3011 - val_loss: 173.7883\n",
            "Epoch 114/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 206.0344 - val_loss: 172.7757\n",
            "Epoch 115/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 206.4577 - val_loss: 172.9823\n",
            "Epoch 116/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 208.0712 - val_loss: 178.2985\n",
            "Epoch 117/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 206.5850 - val_loss: 173.1959\n",
            "Epoch 118/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 205.1442 - val_loss: 171.8375\n",
            "Epoch 119/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 205.8463 - val_loss: 171.5597\n",
            "Epoch 120/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 204.7673 - val_loss: 171.9931\n",
            "Epoch 121/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 204.1830 - val_loss: 171.3195\n",
            "Epoch 122/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 203.6321 - val_loss: 171.3828\n",
            "Epoch 123/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 203.9636 - val_loss: 171.4233\n",
            "Epoch 124/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 204.3806 - val_loss: 170.9846\n",
            "Epoch 125/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 204.1154 - val_loss: 171.3954\n",
            "Epoch 126/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 202.8133 - val_loss: 171.1384\n",
            "Epoch 127/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 202.7143 - val_loss: 169.9295\n",
            "Epoch 128/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 202.4006 - val_loss: 170.0075\n",
            "Epoch 129/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 201.7642 - val_loss: 169.8262\n",
            "Epoch 130/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 201.3489 - val_loss: 169.9992\n",
            "Epoch 131/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 203.6330 - val_loss: 169.6700\n",
            "Epoch 132/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 203.5366 - val_loss: 171.0129\n",
            "Epoch 133/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 201.6088 - val_loss: 170.0613\n",
            "Epoch 134/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 200.9971 - val_loss: 168.5495\n",
            "Epoch 135/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 200.5884 - val_loss: 168.6313\n",
            "Epoch 136/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 199.7181 - val_loss: 168.0331\n",
            "Epoch 137/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 199.4742 - val_loss: 167.7014\n",
            "Epoch 138/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 199.5595 - val_loss: 167.4835\n",
            "Epoch 139/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 200.3952 - val_loss: 166.8135\n",
            "Epoch 140/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 201.4120 - val_loss: 166.9545\n",
            "Epoch 141/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 199.6919 - val_loss: 167.3857\n",
            "Epoch 142/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 197.9266 - val_loss: 166.7722\n",
            "Epoch 143/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 197.5422 - val_loss: 170.1525\n",
            "Epoch 144/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 199.3618 - val_loss: 167.3603\n",
            "Epoch 145/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 197.0680 - val_loss: 166.3529\n",
            "Epoch 146/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 196.3845 - val_loss: 166.0259\n",
            "Epoch 147/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 196.8858 - val_loss: 165.5690\n",
            "Epoch 148/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 196.4025 - val_loss: 166.5199\n",
            "Epoch 149/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 197.2325 - val_loss: 164.5027\n",
            "Epoch 150/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 195.8039 - val_loss: 164.9568\n",
            "Epoch 151/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 194.6650 - val_loss: 164.1683\n",
            "Epoch 152/1400\n",
            "1158/1158 [==============================] - 0s 192us/step - loss: 194.7165 - val_loss: 163.9219\n",
            "Epoch 153/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 193.9606 - val_loss: 163.8510\n",
            "Epoch 154/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 193.6363 - val_loss: 164.2547\n",
            "Epoch 155/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 193.9596 - val_loss: 163.8134\n",
            "Epoch 156/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 193.7708 - val_loss: 164.3951\n",
            "Epoch 157/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 192.8575 - val_loss: 164.6023\n",
            "Epoch 158/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 193.1623 - val_loss: 162.9191\n",
            "Epoch 159/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 193.5174 - val_loss: 162.9497\n",
            "Epoch 160/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 191.4275 - val_loss: 162.7426\n",
            "Epoch 161/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 191.8941 - val_loss: 162.5854\n",
            "Epoch 162/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 191.5272 - val_loss: 162.1685\n",
            "Epoch 163/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 191.1294 - val_loss: 161.9594\n",
            "Epoch 164/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 190.2412 - val_loss: 161.5457\n",
            "Epoch 165/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 190.2498 - val_loss: 162.5270\n",
            "Epoch 166/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 191.9423 - val_loss: 161.9109\n",
            "Epoch 167/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 190.0558 - val_loss: 161.0211\n",
            "Epoch 168/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 189.4736 - val_loss: 162.1106\n",
            "Epoch 169/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 189.1794 - val_loss: 161.4152\n",
            "Epoch 170/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 188.3838 - val_loss: 160.3632\n",
            "Epoch 171/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 187.4404 - val_loss: 159.9455\n",
            "Epoch 172/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 187.1580 - val_loss: 159.5851\n",
            "Epoch 173/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 186.6532 - val_loss: 159.3881\n",
            "Epoch 174/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 186.7934 - val_loss: 159.2662\n",
            "Epoch 175/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 187.1900 - val_loss: 159.6343\n",
            "Epoch 176/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 186.4542 - val_loss: 159.4574\n",
            "Epoch 177/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 185.5234 - val_loss: 158.7455\n",
            "Epoch 178/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 185.2726 - val_loss: 158.4956\n",
            "Epoch 179/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 184.0494 - val_loss: 158.3603\n",
            "Epoch 180/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 184.4672 - val_loss: 158.0345\n",
            "Epoch 181/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 183.7955 - val_loss: 157.5902\n",
            "Epoch 182/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 183.8349 - val_loss: 157.5012\n",
            "Epoch 183/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 182.6554 - val_loss: 158.5508\n",
            "Epoch 184/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 183.4282 - val_loss: 157.0924\n",
            "Epoch 185/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 183.0956 - val_loss: 156.7909\n",
            "Epoch 186/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 183.1215 - val_loss: 158.1331\n",
            "Epoch 187/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 181.6232 - val_loss: 156.2912\n",
            "Epoch 188/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 180.8807 - val_loss: 157.7082\n",
            "Epoch 189/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 181.6698 - val_loss: 157.0160\n",
            "Epoch 190/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 180.2356 - val_loss: 156.2819\n",
            "Epoch 191/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 180.1307 - val_loss: 155.0845\n",
            "Epoch 192/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 179.1423 - val_loss: 155.5874\n",
            "Epoch 193/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 179.8985 - val_loss: 155.6048\n",
            "Epoch 194/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 179.6429 - val_loss: 155.5728\n",
            "Epoch 195/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 178.7895 - val_loss: 155.2607\n",
            "Epoch 196/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 178.0770 - val_loss: 154.4955\n",
            "Epoch 197/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 177.8297 - val_loss: 155.6252\n",
            "Epoch 198/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 179.3836 - val_loss: 154.9270\n",
            "Epoch 199/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 178.2295 - val_loss: 154.9149\n",
            "Epoch 200/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 177.5978 - val_loss: 153.0325\n",
            "Epoch 201/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 175.6615 - val_loss: 155.2509\n",
            "Epoch 202/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 174.7947 - val_loss: 156.1295\n",
            "Epoch 203/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 176.6618 - val_loss: 152.8515\n",
            "Epoch 204/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 174.8921 - val_loss: 152.0347\n",
            "Epoch 205/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 175.4752 - val_loss: 153.7313\n",
            "Epoch 206/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 174.5354 - val_loss: 151.5844\n",
            "Epoch 207/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 173.9288 - val_loss: 151.1830\n",
            "Epoch 208/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 174.8651 - val_loss: 151.8987\n",
            "Epoch 209/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 173.5218 - val_loss: 150.7366\n",
            "Epoch 210/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 172.4118 - val_loss: 150.3549\n",
            "Epoch 211/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 172.2307 - val_loss: 150.7179\n",
            "Epoch 212/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 172.1066 - val_loss: 150.3123\n",
            "Epoch 213/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 171.6053 - val_loss: 150.4277\n",
            "Epoch 214/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 171.9377 - val_loss: 149.9770\n",
            "Epoch 215/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 170.9018 - val_loss: 149.3122\n",
            "Epoch 216/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 171.2628 - val_loss: 150.4709\n",
            "Epoch 217/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 170.2511 - val_loss: 148.9330\n",
            "Epoch 218/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 169.3161 - val_loss: 148.4445\n",
            "Epoch 219/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 168.8668 - val_loss: 148.3439\n",
            "Epoch 220/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 168.7621 - val_loss: 149.2927\n",
            "Epoch 221/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 168.4975 - val_loss: 148.9059\n",
            "Epoch 222/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 168.0151 - val_loss: 148.2362\n",
            "Epoch 223/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 167.9043 - val_loss: 148.1956\n",
            "Epoch 224/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 167.4171 - val_loss: 147.4931\n",
            "Epoch 225/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 166.7401 - val_loss: 148.7928\n",
            "Epoch 226/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 166.4855 - val_loss: 148.6333\n",
            "Epoch 227/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 166.5133 - val_loss: 146.8085\n",
            "Epoch 228/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 166.2266 - val_loss: 147.8353\n",
            "Epoch 229/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 165.4125 - val_loss: 145.7354\n",
            "Epoch 230/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 165.4889 - val_loss: 145.4281\n",
            "Epoch 231/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 164.7525 - val_loss: 148.2977\n",
            "Epoch 232/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 163.8403 - val_loss: 145.3597\n",
            "Epoch 233/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 164.2409 - val_loss: 145.0201\n",
            "Epoch 234/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 164.3836 - val_loss: 144.6420\n",
            "Epoch 235/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 163.8582 - val_loss: 144.4420\n",
            "Epoch 236/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 162.0648 - val_loss: 143.8109\n",
            "Epoch 237/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 162.5962 - val_loss: 143.1831\n",
            "Epoch 238/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 161.4686 - val_loss: 143.3190\n",
            "Epoch 239/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 161.5248 - val_loss: 143.3537\n",
            "Epoch 240/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 161.5926 - val_loss: 142.8613\n",
            "Epoch 241/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 160.2878 - val_loss: 142.4085\n",
            "Epoch 242/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 160.7876 - val_loss: 142.1200\n",
            "Epoch 243/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 159.9369 - val_loss: 143.7067\n",
            "Epoch 244/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 160.0355 - val_loss: 141.8414\n",
            "Epoch 245/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 159.5945 - val_loss: 141.6515\n",
            "Epoch 246/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 158.0345 - val_loss: 141.4292\n",
            "Epoch 247/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 160.0326 - val_loss: 141.9535\n",
            "Epoch 248/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 157.8001 - val_loss: 140.9206\n",
            "Epoch 249/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 157.5942 - val_loss: 143.6859\n",
            "Epoch 250/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 157.5226 - val_loss: 140.4466\n",
            "Epoch 251/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 157.4452 - val_loss: 144.1333\n",
            "Epoch 252/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 157.3159 - val_loss: 141.4034\n",
            "Epoch 253/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 156.4544 - val_loss: 139.6854\n",
            "Epoch 254/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 156.4089 - val_loss: 139.4872\n",
            "Epoch 255/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 154.9737 - val_loss: 139.0762\n",
            "Epoch 256/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 154.4786 - val_loss: 138.8361\n",
            "Epoch 257/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 154.7246 - val_loss: 138.9896\n",
            "Epoch 258/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 154.7578 - val_loss: 138.3083\n",
            "Epoch 259/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 153.8537 - val_loss: 141.7360\n",
            "Epoch 260/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 153.2042 - val_loss: 138.3062\n",
            "Epoch 261/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 156.0233 - val_loss: 139.9659\n",
            "Epoch 262/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 152.4646 - val_loss: 137.9089\n",
            "Epoch 263/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 153.2802 - val_loss: 139.8374\n",
            "Epoch 264/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 152.5481 - val_loss: 137.2280\n",
            "Epoch 265/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 152.3144 - val_loss: 137.1926\n",
            "Epoch 266/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 151.4936 - val_loss: 137.6979\n",
            "Epoch 267/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 151.9080 - val_loss: 139.9975\n",
            "Epoch 268/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 151.6959 - val_loss: 137.2469\n",
            "Epoch 269/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 149.7451 - val_loss: 136.1908\n",
            "Epoch 270/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 150.5285 - val_loss: 136.1524\n",
            "Epoch 271/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 151.3971 - val_loss: 136.2348\n",
            "Epoch 272/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 149.4178 - val_loss: 136.9583\n",
            "Epoch 273/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 150.2578 - val_loss: 135.3001\n",
            "Epoch 274/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 148.4668 - val_loss: 135.7395\n",
            "Epoch 275/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 148.8272 - val_loss: 136.1241\n",
            "Epoch 276/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 149.1388 - val_loss: 134.9386\n",
            "Epoch 277/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 148.2238 - val_loss: 135.0533\n",
            "Epoch 278/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 147.7949 - val_loss: 136.1955\n",
            "Epoch 279/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 148.1501 - val_loss: 135.1071\n",
            "Epoch 280/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 146.6783 - val_loss: 139.3308\n",
            "Epoch 281/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 147.3622 - val_loss: 134.9629\n",
            "Epoch 282/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 146.9224 - val_loss: 135.1110\n",
            "Epoch 283/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 147.9601 - val_loss: 134.3091\n",
            "Epoch 284/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 146.1108 - val_loss: 133.9618\n",
            "Epoch 285/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 145.9646 - val_loss: 133.9477\n",
            "Epoch 286/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 145.6295 - val_loss: 134.4310\n",
            "Epoch 287/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 145.1386 - val_loss: 134.0518\n",
            "Epoch 288/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 145.7556 - val_loss: 133.9762\n",
            "Epoch 289/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 145.4070 - val_loss: 134.3897\n",
            "Epoch 290/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 145.6323 - val_loss: 136.9984\n",
            "Epoch 291/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 146.6322 - val_loss: 132.9201\n",
            "Epoch 292/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 144.7255 - val_loss: 133.4807\n",
            "Epoch 293/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 145.0475 - val_loss: 134.5577\n",
            "Epoch 294/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 148.2861 - val_loss: 134.4544\n",
            "Epoch 295/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 145.5153 - val_loss: 132.6367\n",
            "Epoch 296/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 143.9926 - val_loss: 132.8950\n",
            "Epoch 297/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 145.1970 - val_loss: 132.9665\n",
            "Epoch 298/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 143.0003 - val_loss: 133.5902\n",
            "Epoch 299/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 143.6992 - val_loss: 133.5073\n",
            "Epoch 300/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 143.6739 - val_loss: 132.0812\n",
            "Epoch 301/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 143.0740 - val_loss: 132.1988\n",
            "Epoch 302/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 142.6349 - val_loss: 135.2044\n",
            "Epoch 303/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 142.9857 - val_loss: 132.2228\n",
            "Epoch 304/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 142.6699 - val_loss: 132.1568\n",
            "Epoch 305/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 142.6482 - val_loss: 131.6980\n",
            "Epoch 306/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 142.6790 - val_loss: 132.2792\n",
            "Epoch 307/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 142.6096 - val_loss: 131.9922\n",
            "Epoch 308/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 142.4524 - val_loss: 132.2111\n",
            "Epoch 309/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 142.0212 - val_loss: 133.8766\n",
            "Epoch 310/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 144.0476 - val_loss: 132.8300\n",
            "Epoch 311/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 141.7711 - val_loss: 131.5961\n",
            "Epoch 312/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 141.5259 - val_loss: 131.1486\n",
            "Epoch 313/1400\n",
            "1158/1158 [==============================] - 0s 202us/step - loss: 141.5989 - val_loss: 131.9687\n",
            "Epoch 314/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 143.0757 - val_loss: 131.3183\n",
            "Epoch 315/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 141.0483 - val_loss: 133.2947\n",
            "Epoch 316/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 146.1893 - val_loss: 130.6852\n",
            "Epoch 317/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 140.5420 - val_loss: 131.7999\n",
            "Epoch 318/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 140.3468 - val_loss: 131.3380\n",
            "Epoch 319/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 141.3994 - val_loss: 130.5881\n",
            "Epoch 320/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 140.3023 - val_loss: 131.1025\n",
            "Epoch 321/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 141.4746 - val_loss: 130.4346\n",
            "Epoch 322/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 140.7268 - val_loss: 130.1105\n",
            "Epoch 323/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 139.9716 - val_loss: 131.9140\n",
            "Epoch 324/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 142.0290 - val_loss: 137.5129\n",
            "Epoch 325/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 141.6492 - val_loss: 130.4640\n",
            "Epoch 326/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 139.3010 - val_loss: 130.6979\n",
            "Epoch 327/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 139.5226 - val_loss: 130.3758\n",
            "Epoch 328/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 138.8674 - val_loss: 131.3002\n",
            "Epoch 329/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 139.9772 - val_loss: 130.7583\n",
            "Epoch 330/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 138.7085 - val_loss: 131.1435\n",
            "Epoch 331/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 138.5695 - val_loss: 130.4945\n",
            "Epoch 332/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 138.4453 - val_loss: 129.7898\n",
            "Epoch 333/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 140.8259 - val_loss: 130.0656\n",
            "Epoch 334/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 139.2770 - val_loss: 130.1622\n",
            "Epoch 335/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 138.2408 - val_loss: 129.6331\n",
            "Epoch 336/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 138.0516 - val_loss: 129.5172\n",
            "Epoch 337/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 137.6522 - val_loss: 129.5663\n",
            "Epoch 338/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 137.9853 - val_loss: 130.0682\n",
            "Epoch 339/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 138.3788 - val_loss: 129.6647\n",
            "Epoch 340/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 138.5557 - val_loss: 129.8852\n",
            "Epoch 341/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 137.1146 - val_loss: 130.4838\n",
            "Epoch 342/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 137.9211 - val_loss: 129.5366\n",
            "Epoch 343/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 137.1954 - val_loss: 129.6755\n",
            "Epoch 344/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 137.8168 - val_loss: 130.6209\n",
            "Epoch 345/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 137.0228 - val_loss: 130.9144\n",
            "Epoch 346/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 140.0888 - val_loss: 129.6359\n",
            "Epoch 347/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 136.8595 - val_loss: 129.1057\n",
            "Epoch 348/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 136.6582 - val_loss: 129.2962\n",
            "Epoch 349/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 136.6083 - val_loss: 129.1742\n",
            "Epoch 350/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 136.4867 - val_loss: 129.1829\n",
            "Epoch 351/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 136.0089 - val_loss: 128.9711\n",
            "Epoch 352/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 136.7254 - val_loss: 130.2723\n",
            "Epoch 353/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 136.6477 - val_loss: 128.8559\n",
            "Epoch 354/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 136.5174 - val_loss: 129.3963\n",
            "Epoch 355/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 136.0747 - val_loss: 128.9360\n",
            "Epoch 356/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 134.9208 - val_loss: 130.9385\n",
            "Epoch 357/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 136.0570 - val_loss: 128.6324\n",
            "Epoch 358/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 135.2868 - val_loss: 129.2104\n",
            "Epoch 359/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 136.2169 - val_loss: 129.5357\n",
            "Epoch 360/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 135.8373 - val_loss: 130.7732\n",
            "Epoch 361/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 134.9276 - val_loss: 129.6666\n",
            "Epoch 362/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 135.0046 - val_loss: 129.8410\n",
            "Epoch 363/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 134.9069 - val_loss: 128.7909\n",
            "Epoch 364/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 134.5892 - val_loss: 128.0690\n",
            "Epoch 365/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 134.4723 - val_loss: 127.9747\n",
            "Epoch 366/1400\n",
            "1158/1158 [==============================] - 0s 192us/step - loss: 134.6657 - val_loss: 128.1304\n",
            "Epoch 367/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 134.4203 - val_loss: 128.5878\n",
            "Epoch 368/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 134.5966 - val_loss: 128.2290\n",
            "Epoch 369/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 135.9805 - val_loss: 128.0003\n",
            "Epoch 370/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 135.0694 - val_loss: 127.5143\n",
            "Epoch 371/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 134.1573 - val_loss: 128.1004\n",
            "Epoch 372/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 134.9338 - val_loss: 127.9927\n",
            "Epoch 373/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 134.4165 - val_loss: 128.6453\n",
            "Epoch 374/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 133.7420 - val_loss: 128.8095\n",
            "Epoch 375/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 133.2939 - val_loss: 128.4224\n",
            "Epoch 376/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 132.5589 - val_loss: 128.8029\n",
            "Epoch 377/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 133.7943 - val_loss: 127.4041\n",
            "Epoch 378/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 133.1209 - val_loss: 128.7976\n",
            "Epoch 379/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 132.8668 - val_loss: 127.2014\n",
            "Epoch 380/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 133.0008 - val_loss: 129.3251\n",
            "Epoch 381/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 132.6977 - val_loss: 127.6257\n",
            "Epoch 382/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 132.9798 - val_loss: 127.2475\n",
            "Epoch 383/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 132.6055 - val_loss: 127.3946\n",
            "Epoch 384/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 133.5022 - val_loss: 126.3460\n",
            "Epoch 385/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 134.5677 - val_loss: 127.2596\n",
            "Epoch 386/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 131.9853 - val_loss: 126.5402\n",
            "Epoch 387/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 132.5064 - val_loss: 126.6979\n",
            "Epoch 388/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 132.5954 - val_loss: 126.1337\n",
            "Epoch 389/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 131.5465 - val_loss: 126.3654\n",
            "Epoch 390/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 131.7363 - val_loss: 128.9326\n",
            "Epoch 391/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 131.6537 - val_loss: 126.3397\n",
            "Epoch 392/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 132.3504 - val_loss: 127.0365\n",
            "Epoch 393/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 132.7515 - val_loss: 126.8556\n",
            "Epoch 394/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 131.0292 - val_loss: 126.6607\n",
            "Epoch 395/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 131.3163 - val_loss: 128.9236\n",
            "Epoch 396/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 131.2176 - val_loss: 127.0257\n",
            "Epoch 397/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 131.2659 - val_loss: 126.1018\n",
            "Epoch 398/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 130.4530 - val_loss: 125.5052\n",
            "Epoch 399/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 131.6420 - val_loss: 125.5399\n",
            "Epoch 400/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 129.9016 - val_loss: 125.8413\n",
            "Epoch 401/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 130.1458 - val_loss: 128.1690\n",
            "Epoch 402/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 131.8898 - val_loss: 127.6288\n",
            "Epoch 403/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 130.0679 - val_loss: 125.7608\n",
            "Epoch 404/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 129.7708 - val_loss: 124.9841\n",
            "Epoch 405/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 130.0100 - val_loss: 126.3409\n",
            "Epoch 406/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 129.0932 - val_loss: 124.7219\n",
            "Epoch 407/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 129.2493 - val_loss: 125.1600\n",
            "Epoch 408/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 129.4612 - val_loss: 124.7753\n",
            "Epoch 409/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 129.7919 - val_loss: 124.7508\n",
            "Epoch 410/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 128.4049 - val_loss: 126.4950\n",
            "Epoch 411/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 130.1266 - val_loss: 127.3804\n",
            "Epoch 412/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 129.1929 - val_loss: 124.4782\n",
            "Epoch 413/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 130.0698 - val_loss: 125.1848\n",
            "Epoch 414/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 128.3246 - val_loss: 124.8019\n",
            "Epoch 415/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 129.3539 - val_loss: 124.8671\n",
            "Epoch 416/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 128.4920 - val_loss: 124.1013\n",
            "Epoch 417/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 128.6829 - val_loss: 124.8999\n",
            "Epoch 418/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 128.3770 - val_loss: 124.1052\n",
            "Epoch 419/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 127.9552 - val_loss: 123.9799\n",
            "Epoch 420/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 127.6666 - val_loss: 124.5315\n",
            "Epoch 421/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 128.2190 - val_loss: 123.6078\n",
            "Epoch 422/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 127.6552 - val_loss: 123.9900\n",
            "Epoch 423/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 127.3419 - val_loss: 123.7437\n",
            "Epoch 424/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 127.2200 - val_loss: 125.1160\n",
            "Epoch 425/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 127.9475 - val_loss: 125.1433\n",
            "Epoch 426/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 127.1999 - val_loss: 123.1047\n",
            "Epoch 427/1400\n",
            "1158/1158 [==============================] - 0s 204us/step - loss: 127.2859 - val_loss: 123.6801\n",
            "Epoch 428/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 127.1911 - val_loss: 123.4870\n",
            "Epoch 429/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 127.7793 - val_loss: 124.8785\n",
            "Epoch 430/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 126.9083 - val_loss: 123.4853\n",
            "Epoch 431/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 127.4997 - val_loss: 123.1909\n",
            "Epoch 432/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 126.2975 - val_loss: 122.9018\n",
            "Epoch 433/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 126.4896 - val_loss: 123.0514\n",
            "Epoch 434/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 126.3216 - val_loss: 123.2953\n",
            "Epoch 435/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 127.3479 - val_loss: 123.9685\n",
            "Epoch 436/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 126.4565 - val_loss: 123.0260\n",
            "Epoch 437/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 126.8141 - val_loss: 123.1060\n",
            "Epoch 438/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 125.5881 - val_loss: 123.2496\n",
            "Epoch 439/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 126.1169 - val_loss: 122.6369\n",
            "Epoch 440/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 125.8653 - val_loss: 122.7955\n",
            "Epoch 441/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 125.7117 - val_loss: 122.3366\n",
            "Epoch 442/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 125.9005 - val_loss: 122.9288\n",
            "Epoch 443/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 125.7571 - val_loss: 122.2066\n",
            "Epoch 444/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 125.4561 - val_loss: 122.8103\n",
            "Epoch 445/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 125.3645 - val_loss: 122.9554\n",
            "Epoch 446/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 125.0662 - val_loss: 122.6192\n",
            "Epoch 447/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 126.0024 - val_loss: 122.0881\n",
            "Epoch 448/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 124.8903 - val_loss: 122.8317\n",
            "Epoch 449/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 125.0652 - val_loss: 122.1731\n",
            "Epoch 450/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 124.9696 - val_loss: 127.4774\n",
            "Epoch 451/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 125.8884 - val_loss: 123.8534\n",
            "Epoch 452/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 125.2947 - val_loss: 122.9757\n",
            "Epoch 453/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 125.8356 - val_loss: 123.0590\n",
            "Epoch 454/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 124.7603 - val_loss: 122.1850\n",
            "Epoch 455/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 124.7943 - val_loss: 121.7424\n",
            "Epoch 456/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 125.4078 - val_loss: 121.6336\n",
            "Epoch 457/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 123.8719 - val_loss: 122.7198\n",
            "Epoch 458/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 124.3396 - val_loss: 122.3746\n",
            "Epoch 459/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 124.2159 - val_loss: 122.3976\n",
            "Epoch 460/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 124.3409 - val_loss: 122.6510\n",
            "Epoch 461/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 123.8101 - val_loss: 122.1487\n",
            "Epoch 462/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 124.1532 - val_loss: 121.6759\n",
            "Epoch 463/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 123.8558 - val_loss: 122.1044\n",
            "Epoch 464/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 123.4060 - val_loss: 121.2362\n",
            "Epoch 465/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 124.6854 - val_loss: 121.0828\n",
            "Epoch 466/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 123.7843 - val_loss: 120.8792\n",
            "Epoch 467/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 123.6210 - val_loss: 123.2974\n",
            "Epoch 468/1400\n",
            "1158/1158 [==============================] - 0s 201us/step - loss: 123.8060 - val_loss: 122.4049\n",
            "Epoch 469/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 122.7467 - val_loss: 123.0206\n",
            "Epoch 470/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 123.7469 - val_loss: 121.7423\n",
            "Epoch 471/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 123.4942 - val_loss: 120.8445\n",
            "Epoch 472/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 123.8466 - val_loss: 120.7917\n",
            "Epoch 473/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 122.6308 - val_loss: 120.8021\n",
            "Epoch 474/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 122.0279 - val_loss: 122.3321\n",
            "Epoch 475/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 123.2806 - val_loss: 121.1133\n",
            "Epoch 476/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 122.3193 - val_loss: 120.7089\n",
            "Epoch 477/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 122.3402 - val_loss: 120.7123\n",
            "Epoch 478/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 122.2558 - val_loss: 121.0285\n",
            "Epoch 479/1400\n",
            "1158/1158 [==============================] - 0s 210us/step - loss: 122.5073 - val_loss: 120.2668\n",
            "Epoch 480/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 122.0947 - val_loss: 120.8156\n",
            "Epoch 481/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 122.0457 - val_loss: 120.3787\n",
            "Epoch 482/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 121.7833 - val_loss: 121.1468\n",
            "Epoch 483/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 121.8599 - val_loss: 120.0055\n",
            "Epoch 484/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 121.9890 - val_loss: 120.2763\n",
            "Epoch 485/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 121.8985 - val_loss: 120.9309\n",
            "Epoch 486/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 122.2844 - val_loss: 120.0942\n",
            "Epoch 487/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 121.9717 - val_loss: 119.9545\n",
            "Epoch 488/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 121.4884 - val_loss: 120.1708\n",
            "Epoch 489/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 121.1745 - val_loss: 121.6623\n",
            "Epoch 490/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 121.8583 - val_loss: 121.3699\n",
            "Epoch 491/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 121.4736 - val_loss: 120.0132\n",
            "Epoch 492/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 121.7038 - val_loss: 119.9824\n",
            "Epoch 493/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 121.0960 - val_loss: 122.7204\n",
            "Epoch 494/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 120.9083 - val_loss: 119.5294\n",
            "Epoch 495/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 121.3750 - val_loss: 119.2452\n",
            "Epoch 496/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 120.7562 - val_loss: 119.5376\n",
            "Epoch 497/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 121.3950 - val_loss: 120.6663\n",
            "Epoch 498/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 121.2989 - val_loss: 119.3511\n",
            "Epoch 499/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 122.0767 - val_loss: 119.5873\n",
            "Epoch 500/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 120.3132 - val_loss: 119.9064\n",
            "Epoch 501/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 121.1490 - val_loss: 119.0311\n",
            "Epoch 502/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 120.8759 - val_loss: 119.5644\n",
            "Epoch 503/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 121.1902 - val_loss: 119.3809\n",
            "Epoch 504/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 119.7942 - val_loss: 119.4305\n",
            "Epoch 505/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 120.3531 - val_loss: 119.5134\n",
            "Epoch 506/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 120.6708 - val_loss: 119.0661\n",
            "Epoch 507/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 120.5411 - val_loss: 118.5987\n",
            "Epoch 508/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 120.7208 - val_loss: 119.7785\n",
            "Epoch 509/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 121.5425 - val_loss: 118.7377\n",
            "Epoch 510/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 121.5790 - val_loss: 119.0702\n",
            "Epoch 511/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 120.0423 - val_loss: 118.9442\n",
            "Epoch 512/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 119.4867 - val_loss: 118.9256\n",
            "Epoch 513/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 119.5585 - val_loss: 118.8463\n",
            "Epoch 514/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 119.7803 - val_loss: 118.8369\n",
            "Epoch 515/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 119.5296 - val_loss: 119.2613\n",
            "Epoch 516/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 119.5680 - val_loss: 119.0210\n",
            "Epoch 517/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 120.6489 - val_loss: 118.9878\n",
            "Epoch 518/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 120.4254 - val_loss: 120.0844\n",
            "Epoch 519/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 120.4148 - val_loss: 118.9243\n",
            "Epoch 520/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 119.8842 - val_loss: 117.9917\n",
            "Epoch 521/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 119.3266 - val_loss: 118.1560\n",
            "Epoch 522/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 119.1630 - val_loss: 118.1842\n",
            "Epoch 523/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 118.5581 - val_loss: 118.1610\n",
            "Epoch 524/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 119.2531 - val_loss: 117.8211\n",
            "Epoch 525/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 120.0279 - val_loss: 117.5326\n",
            "Epoch 526/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 118.9249 - val_loss: 117.8796\n",
            "Epoch 527/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 118.7814 - val_loss: 118.1212\n",
            "Epoch 528/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 118.9153 - val_loss: 118.5029\n",
            "Epoch 529/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 119.0398 - val_loss: 117.9415\n",
            "Epoch 530/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 119.7268 - val_loss: 118.2638\n",
            "Epoch 531/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 118.5949 - val_loss: 117.4360\n",
            "Epoch 532/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 118.3089 - val_loss: 119.3921\n",
            "Epoch 533/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 119.3460 - val_loss: 118.1757\n",
            "Epoch 534/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 118.3667 - val_loss: 118.4700\n",
            "Epoch 535/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 119.0211 - val_loss: 118.2541\n",
            "Epoch 536/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 119.7501 - val_loss: 118.3473\n",
            "Epoch 537/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 118.4791 - val_loss: 118.4334\n",
            "Epoch 538/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 118.8786 - val_loss: 117.8470\n",
            "Epoch 539/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 118.6287 - val_loss: 117.5717\n",
            "Epoch 540/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 117.8567 - val_loss: 118.3811\n",
            "Epoch 541/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 119.7367 - val_loss: 117.4570\n",
            "Epoch 542/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 118.6822 - val_loss: 117.4677\n",
            "Epoch 543/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 117.9683 - val_loss: 118.3847\n",
            "Epoch 544/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 118.5300 - val_loss: 117.3693\n",
            "Epoch 545/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 117.6328 - val_loss: 116.5472\n",
            "Epoch 546/1400\n",
            "1158/1158 [==============================] - 0s 205us/step - loss: 118.6102 - val_loss: 117.7866\n",
            "Epoch 547/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 118.0857 - val_loss: 117.4427\n",
            "Epoch 548/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 117.4330 - val_loss: 117.0187\n",
            "Epoch 549/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 117.1861 - val_loss: 116.8538\n",
            "Epoch 550/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 117.2681 - val_loss: 117.0046\n",
            "Epoch 551/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 117.2936 - val_loss: 116.6159\n",
            "Epoch 552/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 118.5792 - val_loss: 116.5747\n",
            "Epoch 553/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 118.6804 - val_loss: 121.1958\n",
            "Epoch 554/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 118.9858 - val_loss: 119.4294\n",
            "Epoch 555/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 117.8400 - val_loss: 116.8220\n",
            "Epoch 556/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 117.3970 - val_loss: 116.3390\n",
            "Epoch 557/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 116.8852 - val_loss: 116.5133\n",
            "Epoch 558/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 116.5926 - val_loss: 116.9649\n",
            "Epoch 559/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 117.4523 - val_loss: 116.9778\n",
            "Epoch 560/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 117.0479 - val_loss: 118.5207\n",
            "Epoch 561/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 117.7509 - val_loss: 116.5250\n",
            "Epoch 562/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 116.7742 - val_loss: 116.9044\n",
            "Epoch 563/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 117.5089 - val_loss: 117.0910\n",
            "Epoch 564/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 116.4627 - val_loss: 116.8844\n",
            "Epoch 565/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 116.5843 - val_loss: 116.4310\n",
            "Epoch 566/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 116.6221 - val_loss: 116.2240\n",
            "Epoch 567/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 116.2651 - val_loss: 116.6411\n",
            "Epoch 568/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 117.2553 - val_loss: 116.4184\n",
            "Epoch 569/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 116.1400 - val_loss: 117.5242\n",
            "Epoch 570/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 116.4820 - val_loss: 117.1226\n",
            "Epoch 571/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 116.9245 - val_loss: 116.5785\n",
            "Epoch 572/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 116.8734 - val_loss: 116.2672\n",
            "Epoch 573/1400\n",
            "1158/1158 [==============================] - 0s 145us/step - loss: 116.0980 - val_loss: 115.8694\n",
            "Epoch 574/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 116.4726 - val_loss: 115.6329\n",
            "Epoch 575/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 116.2967 - val_loss: 117.0428\n",
            "Epoch 576/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 116.3514 - val_loss: 115.8437\n",
            "Epoch 577/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 116.7965 - val_loss: 116.7704\n",
            "Epoch 578/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 116.6532 - val_loss: 115.9037\n",
            "Epoch 579/1400\n",
            "1158/1158 [==============================] - 0s 215us/step - loss: 115.9807 - val_loss: 115.8872\n",
            "Epoch 580/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 115.7233 - val_loss: 116.1731\n",
            "Epoch 581/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 116.2755 - val_loss: 116.5988\n",
            "Epoch 582/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 115.8838 - val_loss: 116.1420\n",
            "Epoch 583/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 115.9809 - val_loss: 116.0427\n",
            "Epoch 584/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 116.2046 - val_loss: 116.3168\n",
            "Epoch 585/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 116.1653 - val_loss: 116.2046\n",
            "Epoch 586/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 115.7029 - val_loss: 117.3332\n",
            "Epoch 587/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 115.6416 - val_loss: 115.5273\n",
            "Epoch 588/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 115.9667 - val_loss: 118.7088\n",
            "Epoch 589/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 115.9319 - val_loss: 115.6016\n",
            "Epoch 590/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 115.3257 - val_loss: 115.2185\n",
            "Epoch 591/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 116.1436 - val_loss: 115.2674\n",
            "Epoch 592/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 115.5489 - val_loss: 115.9939\n",
            "Epoch 593/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 115.1945 - val_loss: 115.4667\n",
            "Epoch 594/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 115.3769 - val_loss: 115.7467\n",
            "Epoch 595/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 115.5566 - val_loss: 115.5866\n",
            "Epoch 596/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 115.9722 - val_loss: 115.5840\n",
            "Epoch 597/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 115.9282 - val_loss: 116.2593\n",
            "Epoch 598/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 114.8294 - val_loss: 116.7713\n",
            "Epoch 599/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 115.2586 - val_loss: 116.0236\n",
            "Epoch 600/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 115.7346 - val_loss: 115.2640\n",
            "Epoch 601/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 115.2538 - val_loss: 117.7077\n",
            "Epoch 602/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 114.8963 - val_loss: 116.3057\n",
            "Epoch 603/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 115.9694 - val_loss: 115.6596\n",
            "Epoch 604/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 114.7874 - val_loss: 114.9705\n",
            "Epoch 605/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 115.0526 - val_loss: 115.6194\n",
            "Epoch 606/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 114.7113 - val_loss: 115.0360\n",
            "Epoch 607/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 115.0841 - val_loss: 115.2531\n",
            "Epoch 608/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 116.1528 - val_loss: 115.4449\n",
            "Epoch 609/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 114.4754 - val_loss: 114.9095\n",
            "Epoch 610/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 115.2780 - val_loss: 115.8461\n",
            "Epoch 611/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 115.9822 - val_loss: 115.3612\n",
            "Epoch 612/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 114.4517 - val_loss: 117.5628\n",
            "Epoch 613/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 114.7395 - val_loss: 115.4041\n",
            "Epoch 614/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 114.0645 - val_loss: 115.0232\n",
            "Epoch 615/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 114.2377 - val_loss: 114.8772\n",
            "Epoch 616/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 115.3538 - val_loss: 115.2819\n",
            "Epoch 617/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 114.2381 - val_loss: 114.9048\n",
            "Epoch 618/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 114.6770 - val_loss: 116.0855\n",
            "Epoch 619/1400\n",
            "1158/1158 [==============================] - 0s 192us/step - loss: 115.6937 - val_loss: 115.5160\n",
            "Epoch 620/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 115.1960 - val_loss: 114.8687\n",
            "Epoch 621/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 114.8680 - val_loss: 116.3231\n",
            "Epoch 622/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 113.7682 - val_loss: 115.9836\n",
            "Epoch 623/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 114.0330 - val_loss: 115.7507\n",
            "Epoch 624/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 114.4841 - val_loss: 114.8022\n",
            "Epoch 625/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 114.5296 - val_loss: 115.3534\n",
            "Epoch 626/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 113.6715 - val_loss: 114.7853\n",
            "Epoch 627/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 115.5582 - val_loss: 114.3408\n",
            "Epoch 628/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 114.1757 - val_loss: 114.7687\n",
            "Epoch 629/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 113.5038 - val_loss: 115.1034\n",
            "Epoch 630/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 115.1982 - val_loss: 114.4859\n",
            "Epoch 631/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 113.7409 - val_loss: 114.1820\n",
            "Epoch 632/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 113.7881 - val_loss: 114.0338\n",
            "Epoch 633/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 114.2861 - val_loss: 114.6956\n",
            "Epoch 634/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 113.4643 - val_loss: 114.1772\n",
            "Epoch 635/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 114.6491 - val_loss: 114.8032\n",
            "Epoch 636/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 113.2336 - val_loss: 115.9895\n",
            "Epoch 637/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 115.2678 - val_loss: 114.5947\n",
            "Epoch 638/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 113.4530 - val_loss: 114.9306\n",
            "Epoch 639/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 113.2840 - val_loss: 114.1346\n",
            "Epoch 640/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 113.9214 - val_loss: 114.0677\n",
            "Epoch 641/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 114.0300 - val_loss: 115.1251\n",
            "Epoch 642/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 114.5113 - val_loss: 114.0954\n",
            "Epoch 643/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 113.9717 - val_loss: 114.4727\n",
            "Epoch 644/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 113.4695 - val_loss: 113.9149\n",
            "Epoch 645/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 113.4780 - val_loss: 114.0311\n",
            "Epoch 646/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 113.0276 - val_loss: 113.5124\n",
            "Epoch 647/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 113.3932 - val_loss: 113.9823\n",
            "Epoch 648/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 114.3715 - val_loss: 114.2866\n",
            "Epoch 649/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 113.4668 - val_loss: 113.8435\n",
            "Epoch 650/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 112.9836 - val_loss: 113.5530\n",
            "Epoch 651/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 114.0007 - val_loss: 113.5758\n",
            "Epoch 652/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 113.4406 - val_loss: 116.3031\n",
            "Epoch 653/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 113.2681 - val_loss: 113.8269\n",
            "Epoch 654/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 113.2753 - val_loss: 113.1807\n",
            "Epoch 655/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 112.8926 - val_loss: 114.3870\n",
            "Epoch 656/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 113.1738 - val_loss: 113.6951\n",
            "Epoch 657/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 113.3772 - val_loss: 113.7620\n",
            "Epoch 658/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 112.7684 - val_loss: 113.7025\n",
            "Epoch 659/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 113.9129 - val_loss: 113.8842\n",
            "Epoch 660/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 114.2313 - val_loss: 113.9383\n",
            "Epoch 661/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 112.6553 - val_loss: 113.3257\n",
            "Epoch 662/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 112.3554 - val_loss: 113.9103\n",
            "Epoch 663/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 113.5250 - val_loss: 114.4408\n",
            "Epoch 664/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 112.6955 - val_loss: 114.4864\n",
            "Epoch 665/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 114.6988 - val_loss: 113.0280\n",
            "Epoch 666/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 112.8193 - val_loss: 113.8094\n",
            "Epoch 667/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 112.3786 - val_loss: 113.1603\n",
            "Epoch 668/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 112.8605 - val_loss: 115.4074\n",
            "Epoch 669/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 112.4002 - val_loss: 113.1951\n",
            "Epoch 670/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 112.1490 - val_loss: 113.4926\n",
            "Epoch 671/1400\n",
            "1158/1158 [==============================] - 0s 204us/step - loss: 112.3464 - val_loss: 114.2691\n",
            "Epoch 672/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 112.3578 - val_loss: 113.3939\n",
            "Epoch 673/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 112.6287 - val_loss: 113.3139\n",
            "Epoch 674/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 114.1922 - val_loss: 113.4132\n",
            "Epoch 675/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 112.3843 - val_loss: 112.5999\n",
            "Epoch 676/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 112.4179 - val_loss: 113.9047\n",
            "Epoch 677/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 112.2495 - val_loss: 113.0402\n",
            "Epoch 678/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 112.4137 - val_loss: 113.6764\n",
            "Epoch 679/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 112.6760 - val_loss: 112.8251\n",
            "Epoch 680/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 113.1161 - val_loss: 112.9194\n",
            "Epoch 681/1400\n",
            "1158/1158 [==============================] - 0s 210us/step - loss: 111.6211 - val_loss: 113.9415\n",
            "Epoch 682/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 112.6494 - val_loss: 113.1890\n",
            "Epoch 683/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 111.5970 - val_loss: 113.5740\n",
            "Epoch 684/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 112.3067 - val_loss: 114.3032\n",
            "Epoch 685/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 112.8131 - val_loss: 113.5174\n",
            "Epoch 686/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 112.6598 - val_loss: 112.8881\n",
            "Epoch 687/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 111.9681 - val_loss: 113.3146\n",
            "Epoch 688/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 111.9437 - val_loss: 112.7516\n",
            "Epoch 689/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 112.0152 - val_loss: 112.4941\n",
            "Epoch 690/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 111.6896 - val_loss: 112.7330\n",
            "Epoch 691/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 111.5734 - val_loss: 112.4987\n",
            "Epoch 692/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 112.0715 - val_loss: 112.8020\n",
            "Epoch 693/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 111.9005 - val_loss: 112.6957\n",
            "Epoch 694/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 111.4294 - val_loss: 113.2503\n",
            "Epoch 695/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 111.9319 - val_loss: 114.4398\n",
            "Epoch 696/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 112.1917 - val_loss: 112.5248\n",
            "Epoch 697/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 111.6281 - val_loss: 112.8977\n",
            "Epoch 698/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 111.3437 - val_loss: 112.2509\n",
            "Epoch 699/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 111.5854 - val_loss: 112.5529\n",
            "Epoch 700/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 111.5585 - val_loss: 113.3546\n",
            "Epoch 701/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 111.7986 - val_loss: 112.6007\n",
            "Epoch 702/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 111.8877 - val_loss: 112.2524\n",
            "Epoch 703/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 111.2682 - val_loss: 112.8848\n",
            "Epoch 704/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 111.3288 - val_loss: 112.4943\n",
            "Epoch 705/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 112.2187 - val_loss: 112.2624\n",
            "Epoch 706/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 111.3180 - val_loss: 112.2124\n",
            "Epoch 707/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 112.3033 - val_loss: 112.8828\n",
            "Epoch 708/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 111.4291 - val_loss: 112.5134\n",
            "Epoch 709/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 111.1831 - val_loss: 113.8328\n",
            "Epoch 710/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 111.0677 - val_loss: 112.5603\n",
            "Epoch 711/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 111.2812 - val_loss: 112.9831\n",
            "Epoch 712/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 110.9672 - val_loss: 112.5874\n",
            "Epoch 713/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 111.5299 - val_loss: 112.4165\n",
            "Epoch 714/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 111.4317 - val_loss: 112.9309\n",
            "Epoch 715/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 111.9208 - val_loss: 112.1387\n",
            "Epoch 716/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 111.3893 - val_loss: 112.1861\n",
            "Epoch 717/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 110.9408 - val_loss: 113.2513\n",
            "Epoch 718/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 112.1993 - val_loss: 112.3462\n",
            "Epoch 719/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 110.9249 - val_loss: 112.0473\n",
            "Epoch 720/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 111.9966 - val_loss: 112.2291\n",
            "Epoch 721/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 111.5517 - val_loss: 112.0482\n",
            "Epoch 722/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 110.9647 - val_loss: 112.4355\n",
            "Epoch 723/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 110.7827 - val_loss: 112.6385\n",
            "Epoch 724/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 110.1875 - val_loss: 113.9605\n",
            "Epoch 725/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 111.2071 - val_loss: 112.1903\n",
            "Epoch 726/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 111.6365 - val_loss: 112.6144\n",
            "Epoch 727/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 110.2827 - val_loss: 112.0352\n",
            "Epoch 728/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 110.1231 - val_loss: 112.3252\n",
            "Epoch 729/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 110.3243 - val_loss: 112.6987\n",
            "Epoch 730/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 109.9383 - val_loss: 112.4705\n",
            "Epoch 731/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 110.9134 - val_loss: 113.5340\n",
            "Epoch 732/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 111.4104 - val_loss: 112.4695\n",
            "Epoch 733/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 110.0229 - val_loss: 112.0334\n",
            "Epoch 734/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 110.0532 - val_loss: 114.7822\n",
            "Epoch 735/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 111.1403 - val_loss: 112.3963\n",
            "Epoch 736/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 110.3010 - val_loss: 112.5081\n",
            "Epoch 737/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 110.2079 - val_loss: 112.2525\n",
            "Epoch 738/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 110.0224 - val_loss: 112.2357\n",
            "Epoch 739/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 110.0987 - val_loss: 111.8463\n",
            "Epoch 740/1400\n",
            "1158/1158 [==============================] - 0s 192us/step - loss: 110.0207 - val_loss: 112.9377\n",
            "Epoch 741/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 111.2772 - val_loss: 114.0859\n",
            "Epoch 742/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 110.0483 - val_loss: 112.6228\n",
            "Epoch 743/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 110.1438 - val_loss: 111.9677\n",
            "Epoch 744/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 109.8989 - val_loss: 111.9444\n",
            "Epoch 745/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 110.1623 - val_loss: 115.2416\n",
            "Epoch 746/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 110.3239 - val_loss: 113.4960\n",
            "Epoch 747/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 111.1664 - val_loss: 112.0571\n",
            "Epoch 748/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 109.4943 - val_loss: 112.0628\n",
            "Epoch 749/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 109.2763 - val_loss: 112.9466\n",
            "Epoch 750/1400\n",
            "1158/1158 [==============================] - 0s 206us/step - loss: 110.6258 - val_loss: 111.6044\n",
            "Epoch 751/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 109.7909 - val_loss: 111.7310\n",
            "Epoch 752/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 109.3702 - val_loss: 112.9390\n",
            "Epoch 753/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 111.2323 - val_loss: 112.0032\n",
            "Epoch 754/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 111.5320 - val_loss: 115.0854\n",
            "Epoch 755/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 110.5685 - val_loss: 111.6445\n",
            "Epoch 756/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 109.8272 - val_loss: 112.6332\n",
            "Epoch 757/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 110.1315 - val_loss: 113.0662\n",
            "Epoch 758/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 109.7194 - val_loss: 111.9446\n",
            "Epoch 759/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 109.3905 - val_loss: 112.4233\n",
            "Epoch 760/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 109.3828 - val_loss: 111.7648\n",
            "Epoch 761/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 109.9865 - val_loss: 112.6186\n",
            "Epoch 762/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 110.1596 - val_loss: 113.2638\n",
            "Epoch 763/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 110.0593 - val_loss: 112.2577\n",
            "Epoch 764/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 109.5800 - val_loss: 111.9041\n",
            "Epoch 765/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 109.0336 - val_loss: 111.9644\n",
            "Epoch 766/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 109.3492 - val_loss: 112.2497\n",
            "Epoch 767/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 109.6001 - val_loss: 112.0493\n",
            "Epoch 768/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 109.3324 - val_loss: 111.8843\n",
            "Epoch 769/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 108.8406 - val_loss: 111.6142\n",
            "Epoch 770/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 109.4669 - val_loss: 112.8036\n",
            "Epoch 771/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 109.1881 - val_loss: 114.5755\n",
            "Epoch 772/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 109.6633 - val_loss: 111.5719\n",
            "Epoch 773/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 111.5459 - val_loss: 113.8980\n",
            "Epoch 774/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 109.3780 - val_loss: 111.8316\n",
            "Epoch 775/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 109.2849 - val_loss: 111.4258\n",
            "Epoch 776/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 108.4790 - val_loss: 111.3678\n",
            "Epoch 777/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 108.4759 - val_loss: 112.7617\n",
            "Epoch 778/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 110.4326 - val_loss: 111.5359\n",
            "Epoch 779/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 108.7885 - val_loss: 111.5146\n",
            "Epoch 780/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 109.7587 - val_loss: 111.6297\n",
            "Epoch 781/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 108.7700 - val_loss: 111.4347\n",
            "Epoch 782/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 108.3875 - val_loss: 112.3764\n",
            "Epoch 783/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 109.0628 - val_loss: 111.7177\n",
            "Epoch 784/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 108.8572 - val_loss: 111.3772\n",
            "Epoch 785/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 109.5468 - val_loss: 112.5896\n",
            "Epoch 786/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 109.0623 - val_loss: 111.6101\n",
            "Epoch 787/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 109.1198 - val_loss: 113.4806\n",
            "Epoch 788/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 109.0756 - val_loss: 111.6304\n",
            "Epoch 789/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 109.0288 - val_loss: 111.4673\n",
            "Epoch 790/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 108.9396 - val_loss: 112.1654\n",
            "Epoch 791/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 109.1188 - val_loss: 111.4857\n",
            "Epoch 792/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 108.9769 - val_loss: 115.5768\n",
            "Epoch 793/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 109.4407 - val_loss: 110.7252\n",
            "Epoch 794/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 108.1489 - val_loss: 111.0052\n",
            "Epoch 795/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 108.3612 - val_loss: 111.8979\n",
            "Epoch 796/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 108.4049 - val_loss: 111.4486\n",
            "Epoch 797/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 109.0746 - val_loss: 111.8527\n",
            "Epoch 798/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 108.9087 - val_loss: 111.7115\n",
            "Epoch 799/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 108.6884 - val_loss: 111.1500\n",
            "Epoch 800/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 108.4108 - val_loss: 110.5621\n",
            "Epoch 801/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 108.3156 - val_loss: 110.9173\n",
            "Epoch 802/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 107.5059 - val_loss: 111.3153\n",
            "Epoch 803/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 107.9535 - val_loss: 111.5705\n",
            "Epoch 804/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 107.9724 - val_loss: 110.9438\n",
            "Epoch 805/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 108.5694 - val_loss: 111.0685\n",
            "Epoch 806/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 109.4618 - val_loss: 111.1212\n",
            "Epoch 807/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 107.9856 - val_loss: 113.2770\n",
            "Epoch 808/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 108.2465 - val_loss: 111.5673\n",
            "Epoch 809/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 107.7776 - val_loss: 110.6933\n",
            "Epoch 810/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 108.0411 - val_loss: 110.5056\n",
            "Epoch 811/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 107.8792 - val_loss: 110.8173\n",
            "Epoch 812/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 107.7915 - val_loss: 110.2206\n",
            "Epoch 813/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 107.6592 - val_loss: 110.9928\n",
            "Epoch 814/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 107.8586 - val_loss: 110.5004\n",
            "Epoch 815/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 107.2776 - val_loss: 110.2226\n",
            "Epoch 816/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 107.8075 - val_loss: 110.5590\n",
            "Epoch 817/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 107.7222 - val_loss: 110.7137\n",
            "Epoch 818/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 107.1553 - val_loss: 110.7622\n",
            "Epoch 819/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 107.5685 - val_loss: 110.4051\n",
            "Epoch 820/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 107.5418 - val_loss: 110.3667\n",
            "Epoch 821/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 107.4186 - val_loss: 110.8882\n",
            "Epoch 822/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 107.7367 - val_loss: 111.9260\n",
            "Epoch 823/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 107.9259 - val_loss: 113.1419\n",
            "Epoch 824/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 107.7032 - val_loss: 110.0795\n",
            "Epoch 825/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 108.2531 - val_loss: 111.4132\n",
            "Epoch 826/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 106.9558 - val_loss: 112.6001\n",
            "Epoch 827/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 107.5812 - val_loss: 109.9736\n",
            "Epoch 828/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 107.3261 - val_loss: 110.0199\n",
            "Epoch 829/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 107.0488 - val_loss: 110.2541\n",
            "Epoch 830/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 107.9305 - val_loss: 111.2807\n",
            "Epoch 831/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 109.3681 - val_loss: 109.7311\n",
            "Epoch 832/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 107.8121 - val_loss: 109.6607\n",
            "Epoch 833/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 107.5771 - val_loss: 110.7542\n",
            "Epoch 834/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 107.7410 - val_loss: 109.9895\n",
            "Epoch 835/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 107.2082 - val_loss: 112.8132\n",
            "Epoch 836/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 107.6058 - val_loss: 110.4772\n",
            "Epoch 837/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 107.7454 - val_loss: 109.6870\n",
            "Epoch 838/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 107.2948 - val_loss: 110.3611\n",
            "Epoch 839/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 106.6521 - val_loss: 109.4735\n",
            "Epoch 840/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 106.6386 - val_loss: 109.6368\n",
            "Epoch 841/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 106.7380 - val_loss: 110.0983\n",
            "Epoch 842/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 107.6493 - val_loss: 109.6038\n",
            "Epoch 843/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 106.4857 - val_loss: 110.3064\n",
            "Epoch 844/1400\n",
            "1158/1158 [==============================] - 0s 202us/step - loss: 106.7470 - val_loss: 110.6329\n",
            "Epoch 845/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 107.1496 - val_loss: 109.6335\n",
            "Epoch 846/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 106.4716 - val_loss: 109.9817\n",
            "Epoch 847/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 106.2561 - val_loss: 110.1837\n",
            "Epoch 848/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 106.8439 - val_loss: 110.6165\n",
            "Epoch 849/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 108.5263 - val_loss: 109.5496\n",
            "Epoch 850/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 106.8357 - val_loss: 111.8952\n",
            "Epoch 851/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 108.2255 - val_loss: 109.6684\n",
            "Epoch 852/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 106.6178 - val_loss: 109.3074\n",
            "Epoch 853/1400\n",
            "1158/1158 [==============================] - 0s 208us/step - loss: 107.0370 - val_loss: 112.1929\n",
            "Epoch 854/1400\n",
            "1158/1158 [==============================] - 0s 198us/step - loss: 107.7127 - val_loss: 110.7050\n",
            "Epoch 855/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 106.3533 - val_loss: 109.4576\n",
            "Epoch 856/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 106.7311 - val_loss: 110.5614\n",
            "Epoch 857/1400\n",
            "1158/1158 [==============================] - 0s 145us/step - loss: 106.0150 - val_loss: 109.8132\n",
            "Epoch 858/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 106.6184 - val_loss: 109.6850\n",
            "Epoch 859/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 108.5762 - val_loss: 109.3123\n",
            "Epoch 860/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 106.1877 - val_loss: 110.6545\n",
            "Epoch 861/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 106.6557 - val_loss: 110.6838\n",
            "Epoch 862/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 106.0270 - val_loss: 112.0102\n",
            "Epoch 863/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 107.2312 - val_loss: 110.1007\n",
            "Epoch 864/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 105.8122 - val_loss: 110.1425\n",
            "Epoch 865/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 107.5686 - val_loss: 109.8082\n",
            "Epoch 866/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 105.9582 - val_loss: 110.1144\n",
            "Epoch 867/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 105.9063 - val_loss: 108.8835\n",
            "Epoch 868/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 106.1776 - val_loss: 109.2547\n",
            "Epoch 869/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 106.2768 - val_loss: 109.3369\n",
            "Epoch 870/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 106.0298 - val_loss: 109.7630\n",
            "Epoch 871/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 106.6188 - val_loss: 109.4126\n",
            "Epoch 872/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 107.5440 - val_loss: 109.4805\n",
            "Epoch 873/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 105.7668 - val_loss: 109.1191\n",
            "Epoch 874/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 106.1083 - val_loss: 109.1028\n",
            "Epoch 875/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 106.3301 - val_loss: 108.8866\n",
            "Epoch 876/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 105.3899 - val_loss: 108.8763\n",
            "Epoch 877/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 108.0735 - val_loss: 108.8517\n",
            "Epoch 878/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 105.8649 - val_loss: 109.1283\n",
            "Epoch 879/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 106.1540 - val_loss: 108.6835\n",
            "Epoch 880/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 105.6152 - val_loss: 109.3414\n",
            "Epoch 881/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 106.8495 - val_loss: 111.3806\n",
            "Epoch 882/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 107.3373 - val_loss: 109.3359\n",
            "Epoch 883/1400\n",
            "1158/1158 [==============================] - 0s 192us/step - loss: 105.4826 - val_loss: 109.1330\n",
            "Epoch 884/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 106.2978 - val_loss: 110.2090\n",
            "Epoch 885/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 105.1522 - val_loss: 108.2010\n",
            "Epoch 886/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 105.2095 - val_loss: 108.9367\n",
            "Epoch 887/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 107.2597 - val_loss: 108.6615\n",
            "Epoch 888/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 106.7248 - val_loss: 113.3738\n",
            "Epoch 889/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 108.1054 - val_loss: 110.7648\n",
            "Epoch 890/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 105.9408 - val_loss: 108.4101\n",
            "Epoch 891/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 105.5348 - val_loss: 108.7752\n",
            "Epoch 892/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 104.8286 - val_loss: 108.9781\n",
            "Epoch 893/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 106.2910 - val_loss: 109.4826\n",
            "Epoch 894/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 105.5903 - val_loss: 109.4790\n",
            "Epoch 895/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 105.4107 - val_loss: 109.9666\n",
            "Epoch 896/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 105.6129 - val_loss: 110.4159\n",
            "Epoch 897/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 106.2362 - val_loss: 108.8096\n",
            "Epoch 898/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 104.9826 - val_loss: 108.1628\n",
            "Epoch 899/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 105.4034 - val_loss: 109.4885\n",
            "Epoch 900/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 105.5942 - val_loss: 109.1367\n",
            "Epoch 901/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 105.5754 - val_loss: 108.8064\n",
            "Epoch 902/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 105.6233 - val_loss: 108.3288\n",
            "Epoch 903/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 104.9491 - val_loss: 108.2167\n",
            "Epoch 904/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 105.0779 - val_loss: 108.8785\n",
            "Epoch 905/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 105.2431 - val_loss: 108.1811\n",
            "Epoch 906/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 106.6316 - val_loss: 108.9686\n",
            "Epoch 907/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 104.5534 - val_loss: 110.2277\n",
            "Epoch 908/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 105.4808 - val_loss: 107.8497\n",
            "Epoch 909/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 106.1585 - val_loss: 108.3952\n",
            "Epoch 910/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 105.9260 - val_loss: 108.4618\n",
            "Epoch 911/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 105.3201 - val_loss: 108.8074\n",
            "Epoch 912/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 105.2601 - val_loss: 108.2226\n",
            "Epoch 913/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 104.6226 - val_loss: 108.2157\n",
            "Epoch 914/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 104.8576 - val_loss: 109.3839\n",
            "Epoch 915/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 104.8775 - val_loss: 108.0225\n",
            "Epoch 916/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 104.8984 - val_loss: 107.9208\n",
            "Epoch 917/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 105.0960 - val_loss: 107.6102\n",
            "Epoch 918/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 104.9424 - val_loss: 107.7007\n",
            "Epoch 919/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 104.5238 - val_loss: 107.7216\n",
            "Epoch 920/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 104.5947 - val_loss: 109.3812\n",
            "Epoch 921/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 104.8073 - val_loss: 107.8217\n",
            "Epoch 922/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 105.7048 - val_loss: 112.9776\n",
            "Epoch 923/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 104.6699 - val_loss: 108.3693\n",
            "Epoch 924/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 104.8830 - val_loss: 108.6626\n",
            "Epoch 925/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 104.7060 - val_loss: 107.8258\n",
            "Epoch 926/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 104.4956 - val_loss: 107.4791\n",
            "Epoch 927/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 104.4307 - val_loss: 107.4437\n",
            "Epoch 928/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 104.5518 - val_loss: 107.5383\n",
            "Epoch 929/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 105.7079 - val_loss: 107.6741\n",
            "Epoch 930/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 105.5211 - val_loss: 107.7806\n",
            "Epoch 931/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 105.8270 - val_loss: 107.5010\n",
            "Epoch 932/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 104.7884 - val_loss: 107.6102\n",
            "Epoch 933/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 104.2732 - val_loss: 107.5953\n",
            "Epoch 934/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 104.4314 - val_loss: 107.8407\n",
            "Epoch 935/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 104.0386 - val_loss: 108.5661\n",
            "Epoch 936/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 105.5213 - val_loss: 107.7102\n",
            "Epoch 937/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 104.0917 - val_loss: 108.4113\n",
            "Epoch 938/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 105.0020 - val_loss: 107.4786\n",
            "Epoch 939/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 104.6044 - val_loss: 107.7932\n",
            "Epoch 940/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 103.9126 - val_loss: 107.4062\n",
            "Epoch 941/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 104.7574 - val_loss: 107.4230\n",
            "Epoch 942/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 104.6757 - val_loss: 107.8734\n",
            "Epoch 943/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 104.1028 - val_loss: 107.2352\n",
            "Epoch 944/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 104.5613 - val_loss: 107.3069\n",
            "Epoch 945/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 103.8511 - val_loss: 107.5070\n",
            "Epoch 946/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 104.4306 - val_loss: 107.3832\n",
            "Epoch 947/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 104.2822 - val_loss: 107.1677\n",
            "Epoch 948/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 104.9294 - val_loss: 107.2089\n",
            "Epoch 949/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 104.3859 - val_loss: 107.3500\n",
            "Epoch 950/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 105.1975 - val_loss: 106.9803\n",
            "Epoch 951/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 104.1863 - val_loss: 109.7837\n",
            "Epoch 952/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 104.6092 - val_loss: 107.9900\n",
            "Epoch 953/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 105.1499 - val_loss: 106.8852\n",
            "Epoch 954/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 103.6236 - val_loss: 108.1313\n",
            "Epoch 955/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 104.6146 - val_loss: 107.2575\n",
            "Epoch 956/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 104.4703 - val_loss: 106.6563\n",
            "Epoch 957/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 104.1075 - val_loss: 107.3006\n",
            "Epoch 958/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 104.4496 - val_loss: 107.1722\n",
            "Epoch 959/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 104.3910 - val_loss: 106.9933\n",
            "Epoch 960/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 103.9713 - val_loss: 107.1034\n",
            "Epoch 961/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 105.0765 - val_loss: 110.1058\n",
            "Epoch 962/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 105.0193 - val_loss: 106.8541\n",
            "Epoch 963/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 103.3514 - val_loss: 108.0890\n",
            "Epoch 964/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 104.7960 - val_loss: 107.0743\n",
            "Epoch 965/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 103.9907 - val_loss: 107.3233\n",
            "Epoch 966/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 104.1271 - val_loss: 106.8257\n",
            "Epoch 967/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 103.4172 - val_loss: 106.7769\n",
            "Epoch 968/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 103.1145 - val_loss: 107.2202\n",
            "Epoch 969/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 103.5471 - val_loss: 106.4504\n",
            "Epoch 970/1400\n",
            "1158/1158 [==============================] - 0s 143us/step - loss: 103.5173 - val_loss: 106.7177\n",
            "Epoch 971/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 104.0767 - val_loss: 106.7554\n",
            "Epoch 972/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 104.9134 - val_loss: 107.6885\n",
            "Epoch 973/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 103.6833 - val_loss: 106.5403\n",
            "Epoch 974/1400\n",
            "1158/1158 [==============================] - 0s 194us/step - loss: 103.4284 - val_loss: 108.1165\n",
            "Epoch 975/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 104.9490 - val_loss: 106.8102\n",
            "Epoch 976/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 103.6410 - val_loss: 106.4566\n",
            "Epoch 977/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 103.9711 - val_loss: 107.4818\n",
            "Epoch 978/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 103.8004 - val_loss: 106.4932\n",
            "Epoch 979/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 104.7974 - val_loss: 108.1403\n",
            "Epoch 980/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 104.4164 - val_loss: 106.5775\n",
            "Epoch 981/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 104.4248 - val_loss: 106.6477\n",
            "Epoch 982/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 103.8570 - val_loss: 106.5115\n",
            "Epoch 983/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 103.9949 - val_loss: 106.1337\n",
            "Epoch 984/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 103.6821 - val_loss: 106.2838\n",
            "Epoch 985/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 103.4133 - val_loss: 106.9767\n",
            "Epoch 986/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 103.3583 - val_loss: 107.0513\n",
            "Epoch 987/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 104.4384 - val_loss: 106.7241\n",
            "Epoch 988/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 104.6315 - val_loss: 107.9519\n",
            "Epoch 989/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 103.4823 - val_loss: 106.2635\n",
            "Epoch 990/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 102.6695 - val_loss: 106.2354\n",
            "Epoch 991/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 103.3654 - val_loss: 106.2197\n",
            "Epoch 992/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 103.4902 - val_loss: 106.0584\n",
            "Epoch 993/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 103.2631 - val_loss: 106.5665\n",
            "Epoch 994/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 105.1141 - val_loss: 106.4413\n",
            "Epoch 995/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 103.4700 - val_loss: 106.7936\n",
            "Epoch 996/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 102.8024 - val_loss: 106.1613\n",
            "Epoch 997/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 102.8787 - val_loss: 106.4112\n",
            "Epoch 998/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 102.8663 - val_loss: 106.8596\n",
            "Epoch 999/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 102.9855 - val_loss: 106.4242\n",
            "Epoch 1000/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 103.3064 - val_loss: 108.5738\n",
            "Epoch 1001/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 102.7572 - val_loss: 106.3322\n",
            "Epoch 1002/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 103.3553 - val_loss: 106.8798\n",
            "Epoch 1003/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 103.5271 - val_loss: 106.0412\n",
            "Epoch 1004/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 103.6485 - val_loss: 106.7215\n",
            "Epoch 1005/1400\n",
            "1158/1158 [==============================] - 0s 202us/step - loss: 103.2090 - val_loss: 106.1053\n",
            "Epoch 1006/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 102.9696 - val_loss: 109.9905\n",
            "Epoch 1007/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 104.1172 - val_loss: 106.6350\n",
            "Epoch 1008/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 102.8437 - val_loss: 106.7809\n",
            "Epoch 1009/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 102.9146 - val_loss: 107.0598\n",
            "Epoch 1010/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 103.1920 - val_loss: 107.4629\n",
            "Epoch 1011/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 102.9980 - val_loss: 106.0583\n",
            "Epoch 1012/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 102.6630 - val_loss: 106.4469\n",
            "Epoch 1013/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 103.2548 - val_loss: 106.2094\n",
            "Epoch 1014/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 102.3344 - val_loss: 106.6242\n",
            "Epoch 1015/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 103.3515 - val_loss: 107.5138\n",
            "Epoch 1016/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 104.3319 - val_loss: 106.0270\n",
            "Epoch 1017/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 101.7563 - val_loss: 109.8141\n",
            "Epoch 1018/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 103.3903 - val_loss: 105.8799\n",
            "Epoch 1019/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 102.6154 - val_loss: 105.7744\n",
            "Epoch 1020/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 102.8427 - val_loss: 106.2509\n",
            "Epoch 1021/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 102.5280 - val_loss: 106.5600\n",
            "Epoch 1022/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 102.8842 - val_loss: 110.1110\n",
            "Epoch 1023/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 104.2915 - val_loss: 107.6704\n",
            "Epoch 1024/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 102.5229 - val_loss: 106.4060\n",
            "Epoch 1025/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 102.4360 - val_loss: 106.4163\n",
            "Epoch 1026/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 102.9125 - val_loss: 106.3574\n",
            "Epoch 1027/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 103.1829 - val_loss: 107.4555\n",
            "Epoch 1028/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 102.8701 - val_loss: 107.2642\n",
            "Epoch 1029/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 102.6350 - val_loss: 105.5824\n",
            "Epoch 1030/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 102.2459 - val_loss: 105.5733\n",
            "Epoch 1031/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 102.1140 - val_loss: 106.1463\n",
            "Epoch 1032/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 103.1047 - val_loss: 105.5853\n",
            "Epoch 1033/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 102.7525 - val_loss: 105.9456\n",
            "Epoch 1034/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 102.4227 - val_loss: 106.1482\n",
            "Epoch 1035/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 103.3200 - val_loss: 106.2552\n",
            "Epoch 1036/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 104.2551 - val_loss: 106.7606\n",
            "Epoch 1037/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 102.0315 - val_loss: 105.9285\n",
            "Epoch 1038/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 102.4336 - val_loss: 105.4864\n",
            "Epoch 1039/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 102.0593 - val_loss: 105.3286\n",
            "Epoch 1040/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 103.2068 - val_loss: 106.2710\n",
            "Epoch 1041/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 102.1043 - val_loss: 105.4290\n",
            "Epoch 1042/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 101.9945 - val_loss: 105.4753\n",
            "Epoch 1043/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 102.2378 - val_loss: 105.6479\n",
            "Epoch 1044/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 102.2228 - val_loss: 105.9027\n",
            "Epoch 1045/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 102.3277 - val_loss: 105.0007\n",
            "Epoch 1046/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 101.9402 - val_loss: 106.2022\n",
            "Epoch 1047/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 102.3822 - val_loss: 105.8793\n",
            "Epoch 1048/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 102.4111 - val_loss: 104.9758\n",
            "Epoch 1049/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 103.2760 - val_loss: 105.4339\n",
            "Epoch 1050/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 102.9511 - val_loss: 106.4371\n",
            "Epoch 1051/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 102.5158 - val_loss: 105.1162\n",
            "Epoch 1052/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 102.8270 - val_loss: 105.4217\n",
            "Epoch 1053/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 102.1148 - val_loss: 107.7874\n",
            "Epoch 1054/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 105.3486 - val_loss: 107.8456\n",
            "Epoch 1055/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 101.8573 - val_loss: 105.3779\n",
            "Epoch 1056/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 102.2209 - val_loss: 105.3222\n",
            "Epoch 1057/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 102.1856 - val_loss: 105.5700\n",
            "Epoch 1058/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 102.1509 - val_loss: 106.2014\n",
            "Epoch 1059/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 102.8440 - val_loss: 107.1441\n",
            "Epoch 1060/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 103.3461 - val_loss: 107.9941\n",
            "Epoch 1061/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 103.8283 - val_loss: 105.0014\n",
            "Epoch 1062/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 102.0670 - val_loss: 104.8245\n",
            "Epoch 1063/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 103.3974 - val_loss: 106.2960\n",
            "Epoch 1064/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 101.7712 - val_loss: 105.7432\n",
            "Epoch 1065/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 101.9961 - val_loss: 105.1243\n",
            "Epoch 1066/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 102.6506 - val_loss: 105.2668\n",
            "Epoch 1067/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 101.9760 - val_loss: 105.0136\n",
            "Epoch 1068/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 101.7318 - val_loss: 105.0820\n",
            "Epoch 1069/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 102.5076 - val_loss: 104.5327\n",
            "Epoch 1070/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 102.2575 - val_loss: 105.5495\n",
            "Epoch 1071/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 102.3712 - val_loss: 104.7834\n",
            "Epoch 1072/1400\n",
            "1158/1158 [==============================] - 0s 145us/step - loss: 101.3899 - val_loss: 105.1745\n",
            "Epoch 1073/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 101.4376 - val_loss: 105.2316\n",
            "Epoch 1074/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 102.0993 - val_loss: 105.2593\n",
            "Epoch 1075/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 101.8920 - val_loss: 106.4583\n",
            "Epoch 1076/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 101.9730 - val_loss: 106.3327\n",
            "Epoch 1077/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 102.7075 - val_loss: 105.8786\n",
            "Epoch 1078/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 103.2029 - val_loss: 105.5590\n",
            "Epoch 1079/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 101.1418 - val_loss: 104.5251\n",
            "Epoch 1080/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 101.4989 - val_loss: 104.7132\n",
            "Epoch 1081/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 101.4636 - val_loss: 105.3688\n",
            "Epoch 1082/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 101.6495 - val_loss: 104.7958\n",
            "Epoch 1083/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 102.1575 - val_loss: 105.2640\n",
            "Epoch 1084/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 101.0438 - val_loss: 104.9554\n",
            "Epoch 1085/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 101.7187 - val_loss: 104.7944\n",
            "Epoch 1086/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 101.1671 - val_loss: 108.0444\n",
            "Epoch 1087/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 101.9379 - val_loss: 104.6623\n",
            "Epoch 1088/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 102.1314 - val_loss: 105.8701\n",
            "Epoch 1089/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 102.1798 - val_loss: 105.3320\n",
            "Epoch 1090/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 101.1025 - val_loss: 107.1140\n",
            "Epoch 1091/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 103.5292 - val_loss: 107.2631\n",
            "Epoch 1092/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 101.3270 - val_loss: 104.3967\n",
            "Epoch 1093/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 101.3149 - val_loss: 104.5424\n",
            "Epoch 1094/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 101.4000 - val_loss: 104.8936\n",
            "Epoch 1095/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 101.2050 - val_loss: 104.4621\n",
            "Epoch 1096/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 101.2436 - val_loss: 107.4119\n",
            "Epoch 1097/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 101.7895 - val_loss: 104.8714\n",
            "Epoch 1098/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 101.2641 - val_loss: 104.5606\n",
            "Epoch 1099/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 101.6777 - val_loss: 104.6983\n",
            "Epoch 1100/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 101.2063 - val_loss: 104.2188\n",
            "Epoch 1101/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 100.9274 - val_loss: 104.4865\n",
            "Epoch 1102/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 101.5089 - val_loss: 104.8424\n",
            "Epoch 1103/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 104.1933 - val_loss: 104.6657\n",
            "Epoch 1104/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 101.3867 - val_loss: 105.1788\n",
            "Epoch 1105/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 101.0897 - val_loss: 104.7594\n",
            "Epoch 1106/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 101.3247 - val_loss: 104.4013\n",
            "Epoch 1107/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 101.6780 - val_loss: 104.4692\n",
            "Epoch 1108/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 101.1802 - val_loss: 105.9100\n",
            "Epoch 1109/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 100.3144 - val_loss: 104.7720\n",
            "Epoch 1110/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 101.3472 - val_loss: 104.3107\n",
            "Epoch 1111/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 101.5489 - val_loss: 104.6052\n",
            "Epoch 1112/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 101.4259 - val_loss: 104.8575\n",
            "Epoch 1113/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 100.9335 - val_loss: 104.6602\n",
            "Epoch 1114/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 100.7736 - val_loss: 104.9216\n",
            "Epoch 1115/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 102.4000 - val_loss: 105.6846\n",
            "Epoch 1116/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 101.7075 - val_loss: 105.6459\n",
            "Epoch 1117/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 101.1535 - val_loss: 105.2777\n",
            "Epoch 1118/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 100.8907 - val_loss: 104.9878\n",
            "Epoch 1119/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 101.4229 - val_loss: 104.0816\n",
            "Epoch 1120/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 101.0694 - val_loss: 104.6783\n",
            "Epoch 1121/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 100.7734 - val_loss: 104.4482\n",
            "Epoch 1122/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 101.0989 - val_loss: 104.2724\n",
            "Epoch 1123/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 100.9343 - val_loss: 104.4440\n",
            "Epoch 1124/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 100.5055 - val_loss: 104.3997\n",
            "Epoch 1125/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 100.9167 - val_loss: 104.6445\n",
            "Epoch 1126/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 101.2278 - val_loss: 104.4979\n",
            "Epoch 1127/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 101.1429 - val_loss: 104.1554\n",
            "Epoch 1128/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 101.1039 - val_loss: 104.3392\n",
            "Epoch 1129/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 100.9528 - val_loss: 104.2204\n",
            "Epoch 1130/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 101.3622 - val_loss: 103.9922\n",
            "Epoch 1131/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 100.9833 - val_loss: 104.6475\n",
            "Epoch 1132/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 100.7020 - val_loss: 104.1394\n",
            "Epoch 1133/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 101.4897 - val_loss: 104.2203\n",
            "Epoch 1134/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 101.0805 - val_loss: 103.8026\n",
            "Epoch 1135/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 101.1824 - val_loss: 104.0449\n",
            "Epoch 1136/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 100.8243 - val_loss: 103.9933\n",
            "Epoch 1137/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 101.2250 - val_loss: 104.2416\n",
            "Epoch 1138/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 102.3131 - val_loss: 105.0036\n",
            "Epoch 1139/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 101.8740 - val_loss: 108.6938\n",
            "Epoch 1140/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 101.0915 - val_loss: 105.0069\n",
            "Epoch 1141/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 100.3265 - val_loss: 104.0086\n",
            "Epoch 1142/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 101.0506 - val_loss: 103.7704\n",
            "Epoch 1143/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 100.8834 - val_loss: 104.6371\n",
            "Epoch 1144/1400\n",
            "1158/1158 [==============================] - 0s 142us/step - loss: 100.7720 - val_loss: 104.1573\n",
            "Epoch 1145/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 101.8091 - val_loss: 103.8768\n",
            "Epoch 1146/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 100.2471 - val_loss: 104.0976\n",
            "Epoch 1147/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 101.6664 - val_loss: 103.7003\n",
            "Epoch 1148/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 100.3257 - val_loss: 104.3302\n",
            "Epoch 1149/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 100.2379 - val_loss: 104.6016\n",
            "Epoch 1150/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 101.2148 - val_loss: 104.0079\n",
            "Epoch 1151/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 100.2066 - val_loss: 106.3893\n",
            "Epoch 1152/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 101.4130 - val_loss: 103.8758\n",
            "Epoch 1153/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 101.0357 - val_loss: 103.6593\n",
            "Epoch 1154/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 100.2744 - val_loss: 104.2873\n",
            "Epoch 1155/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 100.5684 - val_loss: 104.5749\n",
            "Epoch 1156/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 101.4564 - val_loss: 105.6621\n",
            "Epoch 1157/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 100.6099 - val_loss: 103.5455\n",
            "Epoch 1158/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 100.6954 - val_loss: 104.5370\n",
            "Epoch 1159/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 100.4904 - val_loss: 104.7590\n",
            "Epoch 1160/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 100.7056 - val_loss: 104.4970\n",
            "Epoch 1161/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 100.4904 - val_loss: 103.7587\n",
            "Epoch 1162/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 100.4751 - val_loss: 104.4005\n",
            "Epoch 1163/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 100.5279 - val_loss: 103.6452\n",
            "Epoch 1164/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 99.8005 - val_loss: 104.0490\n",
            "Epoch 1165/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 100.1180 - val_loss: 103.6745\n",
            "Epoch 1166/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 101.4904 - val_loss: 103.5866\n",
            "Epoch 1167/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 99.9968 - val_loss: 103.6050\n",
            "Epoch 1168/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 100.7266 - val_loss: 104.0319\n",
            "Epoch 1169/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 100.2348 - val_loss: 103.9003\n",
            "Epoch 1170/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 101.2192 - val_loss: 103.3475\n",
            "Epoch 1171/1400\n",
            "1158/1158 [==============================] - 0s 183us/step - loss: 101.3654 - val_loss: 103.5413\n",
            "Epoch 1172/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 100.1250 - val_loss: 103.5769\n",
            "Epoch 1173/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 100.5007 - val_loss: 104.0042\n",
            "Epoch 1174/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 99.8636 - val_loss: 103.5015\n",
            "Epoch 1175/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 100.5285 - val_loss: 103.4454\n",
            "Epoch 1176/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 99.5010 - val_loss: 107.2032\n",
            "Epoch 1177/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 101.1519 - val_loss: 103.6914\n",
            "Epoch 1178/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 100.3808 - val_loss: 102.9626\n",
            "Epoch 1179/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 100.0224 - val_loss: 106.2751\n",
            "Epoch 1180/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 101.5610 - val_loss: 103.1302\n",
            "Epoch 1181/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 99.5340 - val_loss: 104.0200\n",
            "Epoch 1182/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 100.1742 - val_loss: 104.2505\n",
            "Epoch 1183/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 100.5729 - val_loss: 103.4607\n",
            "Epoch 1184/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 99.6851 - val_loss: 103.3633\n",
            "Epoch 1185/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 100.2587 - val_loss: 104.0180\n",
            "Epoch 1186/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 99.8193 - val_loss: 103.8880\n",
            "Epoch 1187/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 100.1522 - val_loss: 103.7457\n",
            "Epoch 1188/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 100.8072 - val_loss: 103.4404\n",
            "Epoch 1189/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 99.7066 - val_loss: 103.6911\n",
            "Epoch 1190/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 99.9886 - val_loss: 103.3608\n",
            "Epoch 1191/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 99.8587 - val_loss: 103.4511\n",
            "Epoch 1192/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 99.7300 - val_loss: 103.4752\n",
            "Epoch 1193/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 99.7386 - val_loss: 103.5339\n",
            "Epoch 1194/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 100.3146 - val_loss: 103.9411\n",
            "Epoch 1195/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 100.2764 - val_loss: 103.7401\n",
            "Epoch 1196/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 100.7058 - val_loss: 103.6872\n",
            "Epoch 1197/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 99.8248 - val_loss: 103.2217\n",
            "Epoch 1198/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 99.5182 - val_loss: 103.4779\n",
            "Epoch 1199/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 99.7402 - val_loss: 103.5506\n",
            "Epoch 1200/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 101.3284 - val_loss: 104.3076\n",
            "Epoch 1201/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 99.7401 - val_loss: 103.3380\n",
            "Epoch 1202/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 99.5743 - val_loss: 103.3895\n",
            "Epoch 1203/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 100.0011 - val_loss: 103.2325\n",
            "Epoch 1204/1400\n",
            "1158/1158 [==============================] - 0s 166us/step - loss: 99.8337 - val_loss: 103.0775\n",
            "Epoch 1205/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 100.5695 - val_loss: 104.3327\n",
            "Epoch 1206/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 99.6109 - val_loss: 103.1988\n",
            "Epoch 1207/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 99.5980 - val_loss: 104.9788\n",
            "Epoch 1208/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 99.7656 - val_loss: 103.2827\n",
            "Epoch 1209/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 99.5068 - val_loss: 103.1740\n",
            "Epoch 1210/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 99.4222 - val_loss: 103.4986\n",
            "Epoch 1211/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 100.6761 - val_loss: 103.9405\n",
            "Epoch 1212/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 99.2359 - val_loss: 104.1525\n",
            "Epoch 1213/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 98.8295 - val_loss: 104.5069\n",
            "Epoch 1214/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 99.6183 - val_loss: 103.8151\n",
            "Epoch 1215/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 100.2416 - val_loss: 104.7164\n",
            "Epoch 1216/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 99.0928 - val_loss: 103.4411\n",
            "Epoch 1217/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 99.3852 - val_loss: 104.3643\n",
            "Epoch 1218/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 99.7419 - val_loss: 104.0673\n",
            "Epoch 1219/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 99.2585 - val_loss: 104.8846\n",
            "Epoch 1220/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 99.9268 - val_loss: 103.5921\n",
            "Epoch 1221/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 98.7691 - val_loss: 104.3149\n",
            "Epoch 1222/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 99.5968 - val_loss: 104.0179\n",
            "Epoch 1223/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 99.3152 - val_loss: 104.5503\n",
            "Epoch 1224/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 101.4813 - val_loss: 106.0291\n",
            "Epoch 1225/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 101.1239 - val_loss: 103.6000\n",
            "Epoch 1226/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 98.9856 - val_loss: 103.3575\n",
            "Epoch 1227/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 99.6796 - val_loss: 103.1316\n",
            "Epoch 1228/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 99.6779 - val_loss: 103.9964\n",
            "Epoch 1229/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 99.8599 - val_loss: 103.0397\n",
            "Epoch 1230/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 99.4994 - val_loss: 103.7862\n",
            "Epoch 1231/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 99.9771 - val_loss: 102.7741\n",
            "Epoch 1232/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 99.2773 - val_loss: 105.0845\n",
            "Epoch 1233/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 99.8846 - val_loss: 103.6858\n",
            "Epoch 1234/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 99.0612 - val_loss: 103.2637\n",
            "Epoch 1235/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 98.8646 - val_loss: 103.4989\n",
            "Epoch 1236/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 99.4347 - val_loss: 103.0722\n",
            "Epoch 1237/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 100.2999 - val_loss: 102.6514\n",
            "Epoch 1238/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 99.7750 - val_loss: 102.6252\n",
            "Epoch 1239/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 99.2397 - val_loss: 102.9650\n",
            "Epoch 1240/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 98.7165 - val_loss: 102.4701\n",
            "Epoch 1241/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 99.2725 - val_loss: 102.7485\n",
            "Epoch 1242/1400\n",
            "1158/1158 [==============================] - 0s 199us/step - loss: 99.7288 - val_loss: 102.8804\n",
            "Epoch 1243/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 98.9207 - val_loss: 102.2578\n",
            "Epoch 1244/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 98.5592 - val_loss: 103.3983\n",
            "Epoch 1245/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 99.7937 - val_loss: 103.4513\n",
            "Epoch 1246/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 99.4918 - val_loss: 102.4037\n",
            "Epoch 1247/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 98.9267 - val_loss: 102.8805\n",
            "Epoch 1248/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 99.6653 - val_loss: 103.8585\n",
            "Epoch 1249/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 99.0805 - val_loss: 104.0077\n",
            "Epoch 1250/1400\n",
            "1158/1158 [==============================] - 0s 148us/step - loss: 99.3983 - val_loss: 102.9503\n",
            "Epoch 1251/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 99.1302 - val_loss: 102.8730\n",
            "Epoch 1252/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 98.6143 - val_loss: 103.5261\n",
            "Epoch 1253/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 98.6024 - val_loss: 102.7300\n",
            "Epoch 1254/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 99.0381 - val_loss: 102.8966\n",
            "Epoch 1255/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 98.5373 - val_loss: 102.5602\n",
            "Epoch 1256/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 98.5564 - val_loss: 103.1690\n",
            "Epoch 1257/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 98.4532 - val_loss: 103.2125\n",
            "Epoch 1258/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 98.6644 - val_loss: 103.3768\n",
            "Epoch 1259/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 99.5941 - val_loss: 103.8061\n",
            "Epoch 1260/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 98.6290 - val_loss: 102.2982\n",
            "Epoch 1261/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 98.9979 - val_loss: 103.3990\n",
            "Epoch 1262/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 98.6878 - val_loss: 102.2921\n",
            "Epoch 1263/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 98.4346 - val_loss: 102.4335\n",
            "Epoch 1264/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 98.7297 - val_loss: 102.4001\n",
            "Epoch 1265/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 98.6541 - val_loss: 103.3735\n",
            "Epoch 1266/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 98.8022 - val_loss: 102.2400\n",
            "Epoch 1267/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 98.3273 - val_loss: 102.9469\n",
            "Epoch 1268/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 99.6473 - val_loss: 102.9690\n",
            "Epoch 1269/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 98.4943 - val_loss: 104.1314\n",
            "Epoch 1270/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 99.3197 - val_loss: 103.5296\n",
            "Epoch 1271/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 99.7169 - val_loss: 103.3647\n",
            "Epoch 1272/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 98.1095 - val_loss: 102.8082\n",
            "Epoch 1273/1400\n",
            "1158/1158 [==============================] - 0s 195us/step - loss: 99.3059 - val_loss: 103.9155\n",
            "Epoch 1274/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 98.6722 - val_loss: 103.8910\n",
            "Epoch 1275/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 98.5461 - val_loss: 102.7207\n",
            "Epoch 1276/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 98.2517 - val_loss: 102.7015\n",
            "Epoch 1277/1400\n",
            "1158/1158 [==============================] - 0s 146us/step - loss: 98.7478 - val_loss: 102.5662\n",
            "Epoch 1278/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 98.7902 - val_loss: 102.9248\n",
            "Epoch 1279/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 98.8150 - val_loss: 104.1481\n",
            "Epoch 1280/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 99.9351 - val_loss: 101.9953\n",
            "Epoch 1281/1400\n",
            "1158/1158 [==============================] - 0s 162us/step - loss: 97.9750 - val_loss: 102.4859\n",
            "Epoch 1282/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 99.4016 - val_loss: 102.5897\n",
            "Epoch 1283/1400\n",
            "1158/1158 [==============================] - 0s 200us/step - loss: 99.7449 - val_loss: 102.6547\n",
            "Epoch 1284/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 100.6550 - val_loss: 102.1877\n",
            "Epoch 1285/1400\n",
            "1158/1158 [==============================] - 0s 163us/step - loss: 98.8425 - val_loss: 102.1165\n",
            "Epoch 1286/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 99.0256 - val_loss: 102.4564\n",
            "Epoch 1287/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 99.3818 - val_loss: 102.2219\n",
            "Epoch 1288/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 99.9231 - val_loss: 101.8124\n",
            "Epoch 1289/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 99.3543 - val_loss: 102.7217\n",
            "Epoch 1290/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 98.6594 - val_loss: 102.2433\n",
            "Epoch 1291/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 98.0457 - val_loss: 101.6907\n",
            "Epoch 1292/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 98.4819 - val_loss: 102.5581\n",
            "Epoch 1293/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 98.2090 - val_loss: 102.1537\n",
            "Epoch 1294/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 98.6804 - val_loss: 101.8016\n",
            "Epoch 1295/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 97.9408 - val_loss: 102.2721\n",
            "Epoch 1296/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 98.1511 - val_loss: 101.7544\n",
            "Epoch 1297/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 98.3782 - val_loss: 102.7035\n",
            "Epoch 1298/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 98.5135 - val_loss: 102.3706\n",
            "Epoch 1299/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 98.8002 - val_loss: 101.6971\n",
            "Epoch 1300/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 97.9264 - val_loss: 101.7875\n",
            "Epoch 1301/1400\n",
            "1158/1158 [==============================] - 0s 189us/step - loss: 98.9001 - val_loss: 101.9729\n",
            "Epoch 1302/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 98.3459 - val_loss: 101.9733\n",
            "Epoch 1303/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 98.9524 - val_loss: 101.9102\n",
            "Epoch 1304/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 97.8509 - val_loss: 101.6818\n",
            "Epoch 1305/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 99.9735 - val_loss: 101.7070\n",
            "Epoch 1306/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 97.9886 - val_loss: 101.9711\n",
            "Epoch 1307/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 97.7205 - val_loss: 101.9288\n",
            "Epoch 1308/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 98.8063 - val_loss: 101.9413\n",
            "Epoch 1309/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 98.3622 - val_loss: 101.4151\n",
            "Epoch 1310/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 98.3688 - val_loss: 101.8673\n",
            "Epoch 1311/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 97.8417 - val_loss: 101.5997\n",
            "Epoch 1312/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 98.0835 - val_loss: 104.4946\n",
            "Epoch 1313/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 98.9321 - val_loss: 101.6128\n",
            "Epoch 1314/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 99.1785 - val_loss: 101.5030\n",
            "Epoch 1315/1400\n",
            "1158/1158 [==============================] - 0s 187us/step - loss: 97.8394 - val_loss: 101.8954\n",
            "Epoch 1316/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 97.8904 - val_loss: 102.3390\n",
            "Epoch 1317/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 98.4230 - val_loss: 102.5414\n",
            "Epoch 1318/1400\n",
            "1158/1158 [==============================] - 0s 171us/step - loss: 97.8284 - val_loss: 101.9435\n",
            "Epoch 1319/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 98.3360 - val_loss: 102.7752\n",
            "Epoch 1320/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 98.1767 - val_loss: 102.1113\n",
            "Epoch 1321/1400\n",
            "1158/1158 [==============================] - 0s 175us/step - loss: 98.3840 - val_loss: 102.9548\n",
            "Epoch 1322/1400\n",
            "1158/1158 [==============================] - 0s 193us/step - loss: 97.7325 - val_loss: 101.8969\n",
            "Epoch 1323/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 98.1870 - val_loss: 101.5706\n",
            "Epoch 1324/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 98.2800 - val_loss: 101.3807\n",
            "Epoch 1325/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 97.9634 - val_loss: 101.7812\n",
            "Epoch 1326/1400\n",
            "1158/1158 [==============================] - 0s 158us/step - loss: 98.3409 - val_loss: 101.9808\n",
            "Epoch 1327/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 97.6092 - val_loss: 102.6289\n",
            "Epoch 1328/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 98.3637 - val_loss: 102.3331\n",
            "Epoch 1329/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 98.1233 - val_loss: 101.9045\n",
            "Epoch 1330/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 98.5146 - val_loss: 102.0423\n",
            "Epoch 1331/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 98.0411 - val_loss: 101.4311\n",
            "Epoch 1332/1400\n",
            "1158/1158 [==============================] - 0s 176us/step - loss: 97.2313 - val_loss: 104.1398\n",
            "Epoch 1333/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 97.8610 - val_loss: 102.1418\n",
            "Epoch 1334/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 98.6854 - val_loss: 101.8239\n",
            "Epoch 1335/1400\n",
            "1158/1158 [==============================] - 0s 188us/step - loss: 97.4416 - val_loss: 101.8067\n",
            "Epoch 1336/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 97.7172 - val_loss: 101.5872\n",
            "Epoch 1337/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 97.8523 - val_loss: 101.9471\n",
            "Epoch 1338/1400\n",
            "1158/1158 [==============================] - 0s 150us/step - loss: 97.9438 - val_loss: 104.0777\n",
            "Epoch 1339/1400\n",
            "1158/1158 [==============================] - 0s 147us/step - loss: 98.3116 - val_loss: 101.1047\n",
            "Epoch 1340/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 98.2430 - val_loss: 101.3323\n",
            "Epoch 1341/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 97.8606 - val_loss: 101.2942\n",
            "Epoch 1342/1400\n",
            "1158/1158 [==============================] - 0s 185us/step - loss: 98.3400 - val_loss: 101.9561\n",
            "Epoch 1343/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 97.0273 - val_loss: 101.6818\n",
            "Epoch 1344/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 97.3662 - val_loss: 102.2903\n",
            "Epoch 1345/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 97.6819 - val_loss: 102.7936\n",
            "Epoch 1346/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 97.8484 - val_loss: 101.4678\n",
            "Epoch 1347/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 99.1280 - val_loss: 100.9300\n",
            "Epoch 1348/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 97.8664 - val_loss: 101.1625\n",
            "Epoch 1349/1400\n",
            "1158/1158 [==============================] - 0s 151us/step - loss: 97.6452 - val_loss: 102.4969\n",
            "Epoch 1350/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 98.9149 - val_loss: 101.3242\n",
            "Epoch 1351/1400\n",
            "1158/1158 [==============================] - 0s 167us/step - loss: 97.6464 - val_loss: 102.1183\n",
            "Epoch 1352/1400\n",
            "1158/1158 [==============================] - 0s 172us/step - loss: 99.3633 - val_loss: 101.3682\n",
            "Epoch 1353/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 97.3772 - val_loss: 101.2909\n",
            "Epoch 1354/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 98.1122 - val_loss: 101.9821\n",
            "Epoch 1355/1400\n",
            "1158/1158 [==============================] - 0s 184us/step - loss: 97.8091 - val_loss: 101.7051\n",
            "Epoch 1356/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 97.9417 - val_loss: 101.6972\n",
            "Epoch 1357/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 98.3584 - val_loss: 100.9468\n",
            "Epoch 1358/1400\n",
            "1158/1158 [==============================] - 0s 159us/step - loss: 97.5002 - val_loss: 101.4300\n",
            "Epoch 1359/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 97.5511 - val_loss: 101.9850\n",
            "Epoch 1360/1400\n",
            "1158/1158 [==============================] - 0s 149us/step - loss: 97.4614 - val_loss: 101.0522\n",
            "Epoch 1361/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 97.6657 - val_loss: 107.4047\n",
            "Epoch 1362/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 98.5212 - val_loss: 101.8666\n",
            "Epoch 1363/1400\n",
            "1158/1158 [==============================] - 0s 186us/step - loss: 98.6151 - val_loss: 101.4642\n",
            "Epoch 1364/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 97.9698 - val_loss: 101.0262\n",
            "Epoch 1365/1400\n",
            "1158/1158 [==============================] - 0s 177us/step - loss: 97.2102 - val_loss: 101.0397\n",
            "Epoch 1366/1400\n",
            "1158/1158 [==============================] - 0s 173us/step - loss: 97.3484 - val_loss: 100.9099\n",
            "Epoch 1367/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 97.5059 - val_loss: 101.3038\n",
            "Epoch 1368/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 97.7251 - val_loss: 101.2413\n",
            "Epoch 1369/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 98.7592 - val_loss: 102.8200\n",
            "Epoch 1370/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 98.6104 - val_loss: 100.9731\n",
            "Epoch 1371/1400\n",
            "1158/1158 [==============================] - 0s 170us/step - loss: 97.6144 - val_loss: 102.2629\n",
            "Epoch 1372/1400\n",
            "1158/1158 [==============================] - 0s 190us/step - loss: 97.3033 - val_loss: 105.7351\n",
            "Epoch 1373/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 98.2773 - val_loss: 101.3642\n",
            "Epoch 1374/1400\n",
            "1158/1158 [==============================] - 0s 178us/step - loss: 97.4845 - val_loss: 100.5377\n",
            "Epoch 1375/1400\n",
            "1158/1158 [==============================] - 0s 179us/step - loss: 96.9589 - val_loss: 101.2078\n",
            "Epoch 1376/1400\n",
            "1158/1158 [==============================] - 0s 174us/step - loss: 97.5527 - val_loss: 102.5020\n",
            "Epoch 1377/1400\n",
            "1158/1158 [==============================] - 0s 180us/step - loss: 98.3395 - val_loss: 101.7365\n",
            "Epoch 1378/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 98.1245 - val_loss: 102.0042\n",
            "Epoch 1379/1400\n",
            "1158/1158 [==============================] - 0s 157us/step - loss: 99.1017 - val_loss: 100.5921\n",
            "Epoch 1380/1400\n",
            "1158/1158 [==============================] - 0s 153us/step - loss: 97.3974 - val_loss: 101.6959\n",
            "Epoch 1381/1400\n",
            "1158/1158 [==============================] - 0s 160us/step - loss: 97.1343 - val_loss: 101.8122\n",
            "Epoch 1382/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 96.9536 - val_loss: 101.3642\n",
            "Epoch 1383/1400\n",
            "1158/1158 [==============================] - 0s 203us/step - loss: 98.7248 - val_loss: 101.8732\n",
            "Epoch 1384/1400\n",
            "1158/1158 [==============================] - 0s 191us/step - loss: 97.3117 - val_loss: 102.3309\n",
            "Epoch 1385/1400\n",
            "1158/1158 [==============================] - 0s 181us/step - loss: 97.5572 - val_loss: 100.9564\n",
            "Epoch 1386/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 97.1141 - val_loss: 101.1139\n",
            "Epoch 1387/1400\n",
            "1158/1158 [==============================] - 0s 154us/step - loss: 97.4117 - val_loss: 101.0418\n",
            "Epoch 1388/1400\n",
            "1158/1158 [==============================] - 0s 165us/step - loss: 97.1204 - val_loss: 101.1707\n",
            "Epoch 1389/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 96.7366 - val_loss: 100.6269\n",
            "Epoch 1390/1400\n",
            "1158/1158 [==============================] - 0s 152us/step - loss: 97.0605 - val_loss: 100.5874\n",
            "Epoch 1391/1400\n",
            "1158/1158 [==============================] - 0s 164us/step - loss: 97.3794 - val_loss: 100.5939\n",
            "Epoch 1392/1400\n",
            "1158/1158 [==============================] - 0s 169us/step - loss: 96.9086 - val_loss: 100.8660\n",
            "Epoch 1393/1400\n",
            "1158/1158 [==============================] - 0s 197us/step - loss: 96.5381 - val_loss: 101.1641\n",
            "Epoch 1394/1400\n",
            "1158/1158 [==============================] - 0s 182us/step - loss: 97.1793 - val_loss: 101.0617\n",
            "Epoch 1395/1400\n",
            "1158/1158 [==============================] - 0s 196us/step - loss: 98.2372 - val_loss: 100.9204\n",
            "Epoch 1396/1400\n",
            "1158/1158 [==============================] - 0s 161us/step - loss: 97.2408 - val_loss: 100.5167\n",
            "Epoch 1397/1400\n",
            "1158/1158 [==============================] - 0s 155us/step - loss: 96.8368 - val_loss: 100.8344\n",
            "Epoch 1398/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 96.6292 - val_loss: 101.3948\n",
            "Epoch 1399/1400\n",
            "1158/1158 [==============================] - 0s 168us/step - loss: 97.8750 - val_loss: 100.9042\n",
            "Epoch 1400/1400\n",
            "1158/1158 [==============================] - 0s 156us/step - loss: 97.4535 - val_loss: 100.2627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcj9WCv8gfWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "f012b609-24e0-4664-b443-6c3b7371e4e5"
      },
      "source": [
        "##Plotting the learning curve \n",
        "\n",
        "tr = hist.history['loss']\n",
        "val_ls = hist.history['val_loss']\n",
        "plt.plot(tr[1:])\n",
        "plt.plot(val_ls[1:])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f10c0410198>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X2QHPV95/H3t3se9lFaPayEkAQS\nIOKIuMyDjmDH53Ig5sHJWXadzwWXionPKXIXnIrvUnUFdtU5D+cqcpfYF1c5dsiZC075TDjHjnWY\nC8aYOJXLGVhsEAgQLA9CEnpYPe/T7Dz09/7o34pBzMzuSrs7u72fV9UwPb/umfl2o5nP9u/X3WPu\njoiILD1RuwsQEZH2UACIiCxRCgARkSVKASAiskQpAEREligFgIjIEqUAEBFZohQAIiJLlAJARGSJ\nyrW7gFZWr17tmzZtancZIiKLypNPPnnE3funWm5BB8CmTZsYGBhodxkiIouKme2ZznLqAhIRWaIU\nACIiS5QCQERkiVIAiIgsUQoAEZElSgEgIrJEKQBERJaoTAbA6ESVL3x/N0/tPdHuUkREFqxMBkCp\nUuNLPxxk5z4FgIhIM5kMgMgMgCTRD96LiDST7QDQ97+ISFOZDAALa5W4EkBEpJlMBkB8eg9AASAi\n0kwmA0BdQCIiU8tkAITvf+0BiIi0kMkAmNwD0Pe/iEhzGQ2A9L6mPiARkaYyGQBxpEFgEZGpZDIA\nTIPAIiJTymQAQNoN5NoDEBFpKsMBYBoDEBFpIbsBEJm6gEREWpgyAMysw8weN7OnzWyXmf1+aN9s\nZo+Z2aCZ/bWZFUJ7MTweDPM31b3WnaF9t5ndMFcrBeoCEhGZynT2ACaAa939XcDlwI1mdg3wR8AX\n3f0S4DjwybD8J4Hjof2LYTnMbCtwM3AZcCPwZ2YWz+bK1IvMdBSQiEgLUwaAp0bCw3y4OXAt8K3Q\nfi/w4TC9PTwmzL/O0sNytgP3ufuEu78KDAJXz8paNJAGwFy9uojI4jetMQAzi83sKeAw8DDwMnDC\n3athkX3A+jC9HtgLEOafBFbVtzd4Tv173WZmA2Y2MDQ0NPM1Ov06OhFMRKSVaQWAu9fc/XJgA+lf\n7e+Yq4Lc/W533+bu2/r7+8/6deLINAYgItLCjI4CcvcTwKPAu4E+M8uFWRuA/WF6P7ARIMxfDhyt\nb2/wnFmnLiARkdamcxRQv5n1helO4APA86RB8NGw2K3Ad8P0jvCYMP+Hnv4pvgO4ORwltBnYAjw+\nWytypsh0KQgRkVZyUy/COuDecMROBNzv7g+Y2XPAfWb2n4GfAl8Ly38N+CszGwSOkR75g7vvMrP7\ngeeAKnC7u9dmd3XeZDoKSESkpSkDwN13Alc0aH+FBkfxuHsJ+FdNXuvzwOdnXubMxWYkyXy8k4jI\n4pTdM4HVBSQi0lI2A6A6wfuSx+mbeKPdlYiILFjZDICJYe6q3MXPDP+43ZWIiCxY2QwA0t8DMNcg\ngIhIM9kMgMnfBEZjACIizWQ0ANLV0h6AiEhzGQ2A0AWEAkBEpJmMBkBYLfUAiYg0lc0ACIPAMGcn\nGouILHrZDADtAYiITCnjAaAxABGRZjIaADoPQERkKhkNgGyulojIbMrmN6XOAxARmVI2AwCdByAi\nMpVsBsDkpSB0OWgRkaYyGwAJpj0AEZEWshkAgGOY9gBERJrKdACoC0hEpLlMB0CkLiARkaYyHQA6\nE1hEpLkMB0BmV01EZFZM+S1pZhvN7FEze87MdpnZ74T23zOz/Wb2VLh9sO45d5rZoJntNrMb6tpv\nDG2DZnbH3KxSyk0ngomItJKbxjJV4Hfd/Sdm1gs8aWYPh3lfdPc/rl/YzLYCNwOXAecDPzCzS8Ps\nLwMfAPYBT5jZDnd/bjZW5EzpHoAGgUVEmpkyANz9AHAgTA+b2fPA+hZP2Q7c5+4TwKtmNghcHeYN\nuvsrAGZ2X1h2jgLAMAWAiEhTM+ooN7NNwBXAY6HpU2a208zuMbMVoW09sLfuaftCW7P2M9/jNjMb\nMLOBoaGhmZT3Ful5AOoCEhFpZtoBYGY9wN8An3b3U8BXgIuBy0n3EP5kNgpy97vdfZu7b+vv7z/7\n18FA5wGIiDQ1nTEAzCxP+uX/DXf/NoC7H6qb/xfAA+HhfmBj3dM3hDZatM86t0hdQCIiLUznKCAD\nvgY87+5fqGtfV7fYR4Bnw/QO4GYzK5rZZmAL8DjwBLDFzDabWYF0oHjH7KzG2zkGOhFMRKSp6ewB\n/ALwa8AzZvZUaPsMcIuZXU56qM1rwG8CuPsuM7ufdHC3Ctzu7jUAM/sU8BAQA/e4+65ZXJe3UBeQ\niEhr0zkK6B+ZvMD+Wz3Y4jmfBz7foP3BVs+bTY66gEREWsns6bI6EUxEpLXsBoD2AEREWspsAKAf\nhBERaSmzAaBBYBGR1rIbADoPQESkpewGgC4FISLSUmYDQEREWstsAKgLSESktewGgH4SUkSkpcwG\nAPo9ABGRljIbAF73XxERebvMBgBmDS9gJCIiqewGgE4EExFpKbMBkP4egAJARKSZ7AaA6VpAIiKt\nZDYAwDDtAIiINJXpAFAXkIhIc5kNgLQLSAEgItJMZgNAewAiIq1lNgDSq4EqAEREmslsAGDaAxAR\naSW7AaBrAYmItDRlAJjZRjN71MyeM7NdZvY7oX2lmT1sZi+F+xWh3czsS2Y2aGY7zezKute6NSz/\nkpndOnerpRPBRESmMp09gCrwu+6+FbgGuN3MtgJ3AI+4+xbgkfAY4CZgS7jdBnwF0sAAPgf8PHA1\n8LnJ0JgThs4DEBFpYcoAcPcD7v6TMD0MPA+sB7YD94bF7gU+HKa3A1/31I+BPjNbB9wAPOzux9z9\nOPAwcOOsrk193URoD0BEpLkZjQGY2SbgCuAxYK27HwizDgJrw/R6YG/d0/aFtmbtZ77HbWY2YGYD\nQ0NDMynvzBci0qUgRESamnYAmFkP8DfAp939VP08d3dm6c9td7/b3be5+7b+/v5zeCVdDFpEpJVp\nBYCZ5Um//L/h7t8OzYdC1w7h/nBo3w9srHv6htDWrH2OaBBYRKSV6RwFZMDXgOfd/Qt1s3YAk0fy\n3Ap8t6794+FooGuAk6Gr6CHgejNbEQZ/rw9tc8N0IpiISCu5aSzzC8CvAc+Y2VOh7TPAXcD9ZvZJ\nYA/wsTDvQeCDwCAwBnwCwN2PmdkfAk+E5f7A3Y/Nylo04DoPQESkpSkDwN3/keYd6tc1WN6B25u8\n1j3APTMp8OwpAEREWsnumcC6GqiISEuZDQCdCSwi0lpmAyDdAxARkWYyHAAR6CggEZGmMhsADkTq\nAhIRaSqzAaATwUREWstuAOgoIBGRlrIbANoDEBFpKbsBEI4Ccg0Ei4g0lNkAcCIM14FAIiJNZDYA\nwIhwdQKJiDSR3QAwwh6AIkBEpJEMB0A6BpDo+19EpKHsBkC4Gqg6gUREGstuAFj6o/DqARIRaSy7\nATA5CKwAEBFpKLsBYOoCEhFpJbsBgAaBRURayW4ATO4BqA9IRKSh7AbA6aOARESkkcwGgFu6ap60\nuRARkQUqswGAQUSifQARkSamDAAzu8fMDpvZs3Vtv2dm+83sqXD7YN28O81s0Mx2m9kNde03hrZB\nM7tj9lflbZVrEFhEpIXp7AH8JXBjg/Yvuvvl4fYggJltBW4GLgvP+TMzi80sBr4M3ARsBW4Jy84d\nDQKLiLSUm2oBd/8HM9s0zdfbDtzn7hPAq2Y2CFwd5g26+ysAZnZfWPa5GVc8TaZBYBGRls5lDOBT\nZrYzdBGtCG3rgb11y+wLbc3a54xbFLqAFAEiIo2cbQB8BbgYuBw4APzJbBVkZreZ2YCZDQwNDZ3L\nK2Hm+lVIEZEmzioA3P2Qu9fcPQH+gje7efYDG+sW3RDamrU3eu273X2bu2/r7+8/m/JSpi4gEZFW\nzioAzGxd3cOPAJNHCO0AbjazopltBrYAjwNPAFvMbLOZFUgHinecfdnTqhHD1QUkItLElIPAZvZN\n4P3AajPbB3wOeL+ZXU7awfIa8JsA7r7LzO4nHdytAre7ey28zqeAh4AYuMfdd8362ry18vCj8HP7\nLiIii9V0jgK6pUHz11os/3ng8w3aHwQenFF15yLsAdSUACIiDWX2TOD0KCD9HoCISDOZDQALPwgj\nIiKNZTYAMAMNAouINJXdANAgsIhIS9kNAB0GKiLSUoYDINKJYCIiLWQ4ANJBYO0AiIg0lt0AQJeD\nFhFpJbMBkF4KQteCExFpJrMBMHkYqHYAREQay24AnP5JSCWAiEgj2Q0As/RH4fX9LyLSUIYDQL8I\nJiLSSmYDwADTELCISFOZDQB0NVARkZYyHAAaBBYRaSXTAYAuBSEi0lRmA8AsCpeCUASIiDSS2QCY\nvBREou9/EZGGshsA4XLQuhiEiEhjmQ0AOz0I3O5KREQWpswGABYRmQ4DFRFpZsoAMLN7zOywmT1b\n17bSzB42s5fC/YrQbmb2JTMbNLOdZnZl3XNuDcu/ZGa3zs3q1NWNAeBJMtdvJSKyKE1nD+AvgRvP\naLsDeMTdtwCPhMcANwFbwu024CuQBgbwOeDngauBz02GxlxxSwNA5wGIiDQ2ZQC4+z8Ax85o3g7c\nG6bvBT5c1/51T/0Y6DOzdcANwMPufszdjwMP8/ZQmVUWAsBdewAiIo2c7RjAWnc/EKYPAmvD9Hpg\nb91y+0Jbs/a5EwJAgwAiIo2d8yCwp2dazdq3rJndZmYDZjYwNDR09q8zOQagABARaehsA+BQ6Noh\n3B8O7fuBjXXLbQhtzdrfxt3vdvdt7r6tv7//LMsDoii8nrqAREQaOdsA2AFMHslzK/DduvaPh6OB\nrgFOhq6ih4DrzWxFGPy9PrTNGdMgsIhIS7mpFjCzbwLvB1ab2T7So3nuAu43s08Ce4CPhcUfBD4I\nDAJjwCcA3P2Ymf0h8ERY7g/c/cyB5VnloQsIHQYqItLQlAHg7rc0mXVdg2UduL3J69wD3DOj6s7B\nm0cBaQ9ARKSRzJ4JHIcxgHK11uZKREQWpswGQDGf7tyMlSttrkREZGHKbADkczEA4xMKABGRRjIb\nAIUQAGPlapsrERFZmDIbAPk4XbXjo+U2VyIisjBlNgDM0lXbtf9EmysREVmYMhsARGkX0PNvnKBa\n07kAIiJnynAApEcBVStlXjg43OZiREQWnuwGQJwHIE+Nn7x+vM3FiIgsPBkOgAIAa3tintyjABAR\nOVN2AyB0Ab1rXZf2AEREGshuAIQuoJ9b18neY+MMDU+0uSARkYUluwEQpQGwdU0XgPYCRETOkN0A\nCGMAF68uko9NASAicoYMB0A6BlC0hK3nL+ene3RCmIhIvewGQOgColbmygv62Ln/BBWdECYiclp2\nAyB0AVEpceUFKyhVEp4/cKq9NYmILCDZDYDVW9JDQff+mKsuXAGg8wFEROpkNwA6++DC98ALD3J+\nXyfrlncoAERE6mQ3AADe8S/gyG4YepGrLlyhABARqZPxAPjl9P6F/822C1dw4GSJN06Mt7cmEZEF\nItsBsHw9nH8F7P47rrpwJQAD2gsQEQHOMQDM7DUze8bMnjKzgdC20sweNrOXwv2K0G5m9iUzGzSz\nnWZ25WyswJS23AD7nuBnl5fpzMf8RAEgIgLMzh7AL7r75e6+LTy+A3jE3bcAj4THADcBW8LtNuAr\ns/DeU7v0esDJvfool2/sY2DPsXl5WxGRhW4uuoC2A/eG6XuBD9e1f91TPwb6zGzdHLz/W627Arr7\n4cWH2LZpBc8fGGZ0Qj8ULyJyrgHgwPfN7Ekzuy20rXX3A2H6ILA2TK8H9tY9d19om1tRBJd8AAZ/\nwFUbe6klztN7dVkIEZFzDYD3uvuVpN07t5vZ++pnuruThsS0mdltZjZgZgNDQ0PnWF5w6fVQOsG2\neBAzDQSLiMA5BoC77w/3h4HvAFcDhya7dsL94bD4fmBj3dM3hLYzX/Nud9/m7tv6+/vPpbw3XXwd\nxEV6Xv4el67p5YnXNA4gInLWAWBm3WbWOzkNXA88C+wAbg2L3Qp8N0zvAD4ejga6BjhZ11U0tzqW\nwZYPwK7v8N6LV/DYq8cYL9fm5a1FRBaqc9kDWAv8o5k9DTwOfM/d/w64C/iAmb0E/FJ4DPAg8Aow\nCPwF8Fvn8N4z93P/EkYOsX3Fq5SrCf/vlSPz+vYiIgtN7myf6O6vAO9q0H4UuK5BuwO3n+37nbNL\nb4B8N5cd/T6d+V/h73cPce071k79PBGRjMr2mcD1Ct2wdTvxc9/h2ou6eHT3YdJMEhFZmpZOAABc\n+XEoj/CrvT9l77FxXjky2u6KRETaZmkFwAXXwKotXHV0BwCPvnB4iieIiGTX0goAM9j2CYoHBtje\nf4CHdh1sd0UiIm2ztAIA0m6gjuX8duEBBvYc5/Bwqd0ViYi0xdILgGIv/LPf4OKjf88mDvC3P33b\nuWgiIkvC0gsAgJ//t1hc4LN9P+Def9pDtZa0uyIRkXm3NAOgZw1c/q+5duIHlE+8wcPPHWp3RSIi\n825pBgDAe34b8xqf7n2EL/7gRSraCxCRJWbpBsCqi7Gf/RAf4/u8cegwX350sN0ViYjMq6UbAADv\n/TT5ygh3bXycP33kJe79p9eoJTo7WESWhrO+FlAmnH8FXHoTv/zK1/m/F1zF53bs4qs/epl/vmU1\n71y/nGWdedb3dbKqp0gtcS5Z00O1lhBHhpm1u3oRkXNiC/l6ONu2bfOBgYG5fZPhQ/DV9+Jm/N3l\nX+aewW6e3neScvXtYwK9HTmGS+nPSb77olWs7C7Q31tkWUeOjkLM6u4iyzpzrO4pcuGqblZ1F4gi\nBYWIzC8ze7Lud9qbL7fkAwDg8Avw9Q/B6BF450dJLngPhwoXMBwv56D3sefAEfYeGWai6zy+/dP9\nbFzRRTEfcWRkguOjFUbLVRptxu5CzJplHfT3Fukp5rhkTQ/r+zo5b3kHq3uKrF1WZE1vB4Xc0u6J\nE5HZpQCYqdGj8KO7YOf9UGrym8ErNkFxGXQsh0IPnHgdLrmOaudqxuIeRq2XkVrM4SNHeKNjC3tG\nY/aP59g3YrxxosSRkQkmGuxZrOwusHFlFxeu7OL8vk5WdRfYtLqbNb1F+nuLrO4pKiREZNoUAGfL\nHY6+DAd3QnkERg7DwWdg7Cj0ngelU1A6CSf2wPA0f9CsuByKvXixh2qum1LUxbh1MkoHJ2udHK3k\neXmkyIu+jt0jPRyu9TBOgQkKVMiRi3Oc39fByu4C5y3vYN3yTlZ2F+guxFy6tpeVPQVWdRdZ0ZU/\nPT5RS5xY3U8iS9J0A2BpDwI3YgarL0lv05EkUB6G8RNpMJROwPDBNEjKIzB2DMaOwMQwNjFMvjxC\nfmKY3om0jfIwlE6++Qs6+XCrc6jzYo57H6dO5Ljs8NN8L3kPBS8x7kW+7xt5zc+jmxLdNs4bhc0s\nT07wQuU8LlvbQb5rOcVCgT3DUI67eV/Pfsp9F5EUeljWkaeYizhVqrJpdTe9xRzFfERHPqYQR+Ri\noyufI3GnmI9Y3VMkH2tPRCQrFADnKorSLqGO5Wf/Gu5w+Lk0OMoj6d5GeSy93z/A2nwXa0cOwdhB\nKJf4WPTDFq8FGFAAjodbvXDS8ynvZII8Bao4RoUcnUwwQiej3sEEBSKq5G2cA76SCjlOMcIwXWAR\nI9ZN1QpMRJ1MRJ10WgWP8kwUVhLn80RRjmWVIXoYY3/PO0niIh02QRxFlPN9RHFMHOeI45juylEq\nxZV4sZc4zoEZ3R0FalEHSVwAi4jjmFwuR7FQoJDPEx/eBWt/jqijh2IuIkcNy3Vg+TwRTq58kuLy\ndVhtAjwhKSyj5p4GWLUMUZze6iU18ATiMxJ4Prinf3yIzCMFwEJgBmsvS2/TkdSgOgG1choSY8eg\nMpaGhzsceRE6+9J5lVJ6AbzyCCRVeOVH1M57J11xkWK5RGI5quUSnhjjnhCXx+mtjNGbVKmSwyZO\nsbE2TlwZxZKIZVGOKCnjfoKVE/spUySq1XCMQlIiX6q8rdwrTzw0yxts5iIAN6oYOUvHYSY8T5WI\nglWZoECRCnmqHGcZOSrkSBi1bsCZoECBKkUqGM7JqI+axXQlo0xYB5AQmeEYHUzgWPpci4i8yprq\nAY7n+qlakRwVziu/zisdlxEZdNRG6asOcbSwnom4m+7qcUr5Piq5LiJ3VpRe51hxA5FXsChPKd9H\n4gn56ihxHEO1TBLniQo9VBKHpEI+qdBTPshI1wVYnKeS68ZrVeI4R5QrsPLYTxjp2cRI94W4RTgG\nRJhXKJRPYsVeOkZeZ2z5JeSK3dTcoTyG5zrIW40k102uMkKufByrlal0rSF/5AXyHV2ML7sIL/RQ\n84h8PgcWUSPiRCmhmM9Ts4jl3Z3kqFEeGybO5SmMvkFkYMUeyvk+TtUK9PV0pIdbm5GPIhLSrs1c\ndQQbOYSvuoQ4ikiqlfTfZL4Lz3UQVceJDj5D3H8J1c5VRGbEkTFRMzryEQak/6ecsXKVWq1Kt5Wx\nnjVEuQ6i2jhJkhDliiTVMlGhC6+MYeH/70TcTSGXI4pjwMCi9DM8egTyHWlbviv9TCbV9LO67Pz0\njwuL0s/oqz+C9VelXcy5AizfmC7XsQyiPEQ5yBWh2DOnnwsFwGIUxVDoArrSL/pVF8/o6ZN/987G\n//xC/QP39JZUwWtQq6QfhNGhNKzCX9e10gjVaoVqtUq1ViUZO8m45yhXKhgJSbVGuVpNP8heoVZL\nSGpVkqRGrTxBtZZQLB+nEhVJEqeSJCQeka8MU7OYuDZO9/gBjhY3kFiOXHUMtyh97VqNJCpQqtYo\nJiViatQSyFlCyXMUQgiUqk6uNo4BOXMST6haIV0mmaCYjBNRYyLqJOdlIhKiWpkyOYrJOHmvMkI3\nkSfgCbFNMOgXkEsqdPsox+wSTpUizIyc9XBe8hrDpQrjjPM6q+krnSTyU0Q4VTNypb04Ticl8AKO\nkaNGbDWOey8xCUXKFIAqOfKUWRMNwcgQNSJ6qNHLGCUK5KjRYyX6Tu2m5pZ+9dtbxwITt7Ttje/P\nwr+S1AUzWHYufq272KCtt8my0Rn3VnffMZtFtfBy4R1c/JnH5vQ9FAAye8Jfa0QhFvKd6f2ydW9Z\nLF6ehlD9B3LFvBTYXudPMf8d03gNd6eaOLEZSZheSZq7pUqNyIxcbFRqCSNxRK1UITbDIuNYNcEM\nxss1hsze/FILE+aAJ5QTp1RJcIdKrUy1UiUXQTGfI0mS9Ei26gSJpX8BJ1Eet5jS+BhRBAVzPKky\nVipjnhBZQiFyhk6Ns6YnJkoSJsplKkRYoZtqtUoucqqeI544QWIxh0+Os7Y3D3ga8uEEzJx5erZ+\nFJPUqlRqTsEq1KwIOLlaiSSpkpSGGaOTro4CNYfEwWu1dBuG7ZWYEVlEdzGmNDZK4hDVSpQtTxxF\neK1CYnmsVmLc80QW0VE9SSEX457WZJ6kvy3uCcsqRyjnehjN9ZFPSiRuxMkEXZVjjOZXYp4wVkno\nzEcUqyOM5vroqw4xbl0kcQfjniOflIiSKpZU6Vl1HjP7027mFAAii4iZkY/Tb+wII1c3jNFZiN+2\nfE9RH3Fpbt4P6TCzG81st5kNmtkd8/3+IiKSmtcAMLMY+DJwE7AVuMXMts5nDSIikprvPYCrgUF3\nf8Xdy8B9wPZ5rkFERJj/AFgP7K17vC+0iYjIPFtwp3Wa2W1mNmBmA0NDQ+0uR0Qks+Y7APYDG+se\nbwhtp7n73e6+zd239ff3z2txIiJLyXwHwBPAFjPbbGYF4GZgxzzXICIizPN5AO5eNbNPAQ+Rngt0\nj7vvms8aREQktaAvB21mQ8Cec3iJ1cCRWSpnri2mWmFx1buYaoXFVe9iqhUWV73nUuuF7j5lH/qC\nDoBzZWYD07km9kKwmGqFxVXvYqoVFle9i6lWWFz1zketC+4oIBERmR8KABGRJSrrAXB3uwuYgcVU\nKyyuehdTrbC46l1MtcLiqnfOa830GICIiDSX9T0AERFpIpMBsBAvOW1mG83sUTN7zsx2mdnvhPaV\nZvawmb0U7leEdjOzL4V12GlmV7ah5tjMfmpmD4THm83ssVDTX4eT+TCzYng8GOZvakOtfWb2LTN7\nwcyeN7N3L9Rta2b/PvwbeNbMvmlmHQtp25rZPWZ22MyerWub8bY0s1vD8i+Z2a3zWOt/Df8OdprZ\nd8ysr27enaHW3WZ2Q137vHxnNKq3bt7vmpmb2erweO63rbtn6kZ6gtnLwEWkv1j4NLB1AdS1Drgy\nTPcCL5JeEvu/AHeE9juAPwrTHwT+D+mv0F0DPNaGmv8D8D+BB8Lj+4Gbw/RXgX8Xpn8L+GqYvhn4\n6zbUei/wG2G6APQtxG1LevHDV4HOum366wtp2wLvA64Enq1rm9G2BFYCr4T7FWF6xTzVej2QC9N/\nVFfr1vB9UAQ2h++JeD6/MxrVG9o3kp4guwdYPV/bdl7+0c/nDXg38FDd4zuBO9tdV4M6vwt8ANgN\nrAtt64DdYfrPgVvqlj+93DzVtwF4BLgWeCD8IzxS98E6vZ3DP9x3h+lcWM7msdbl4UvVzmhfcNuW\nN6+IuzJsqweAGxbatgU2nfGlOqNtCdwC/Hld+1uWm8taz5j3EeAbYfot3wWT23a+vzMa1Qt8C3gX\n8BpvBsCcb9ssdgEt+EtOh934K4DHgLXufiDMOsibv4fd7vX4b8B/BJLweBVwwt2rDeo5XWuYfzIs\nP182A0PA/whdVv/dzLpZgNvW3fcDfwy8Dhwg3VZPsnC37aSZbst2//ud9G9I/4qGBVqrmW0H9rv7\n02fMmvN6sxgAC5qZ9QB/A3za3U/Vz/M0ztt+WJaZ/Qpw2N2fbHct05Qj3a3+irtfAYySdlOctoC2\n7QrSH0HaTPo78d3AjW0taoYWyracipl9FqgC32h3Lc2YWRfwGeA/teP9sxgAU15yul3MLE/65f8N\nd/92aD5kZuvC/HXA4dDezvX4BeBDZvYa6a+2XQv8KdBnZpMXEKyv53StYf5y4Og81QrpX0D73P2x\n8PhbpIGwELftLwGvuvuQu1eAb5Nu74W6bSfNdFu29XNoZr8O/ArwqyGwaFFTO2u9mPSPgafD520D\n8BMzO69FXbNWbxYDYEFectrelDuPAAABfElEQVTMDPga8Ly7f6Fu1g5gchT/VtKxgcn2j4cjAa4B\nTtbtgs8pd7/T3Te4+ybS7fdDd/9V4FHgo01qnVyHj4bl5+0vRHc/COw1s58JTdcBz7EAty1p1881\nZtYV/k1M1rogt22dmW7Lh4DrzWxF2Ou5PrTNOTO7kbT78kPuPnbGOtwcjqzaDGwBHqeN3xnu/oy7\nr3H3TeHzto/0YJGDzMe2nauBjnbeSEfPXyQd2f9su+sJNb2XdLd5J/BUuH2QtD/3EeAl4AfAyrC8\nAV8O6/AMsK1Ndb+fN48Cuoj0AzMI/C+gGNo7wuPBMP+iNtR5OTAQtu/fkh4dsSC3LfD7wAvAs8Bf\nkR6VsmC2LfBN0vGJCukX0ifPZluS9r8Phtsn5rHWQdI+8snP2Vfrlv9sqHU3cFNd+7x8ZzSq94z5\nr/HmIPCcb1udCSwiskRlsQtIRESmQQEgIrJEKQBERJYoBYCIyBKlABARWaIUACIiS5QCQERkiVIA\niIgsUf8fqWWK333bdmoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gryPzko7gjTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2435633-ec05-4476-b295-80d1de54c9eb"
      },
      "source": [
        "##Estimating the Root Mean Square Error of the Model - A metric of its performance\n",
        "\n",
        "predicted = model.predict([x_test_t])\n",
        "y_test_t=np.reshape(y_test_t, (np.shape(y_test_t)[0],1))\n",
        "\n",
        "actual =[]\n",
        "pred=[]\n",
        "se=0\n",
        "show = np.hstack((y_test_t, predicted))\n",
        "\n",
        "for i in show:\n",
        "  se+=(i[0]-i[1])**2\n",
        "  actual.append(i[0])\n",
        "  pred.append(i[1])\n",
        "  \n",
        "rmse = (se/np.size(y_test_t))**0.5\n",
        "print(rmse)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.73650898891647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZFQGvBiOxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "70d380fa-98a3-447f-c644-deb52bae853f"
      },
      "source": [
        "plt.plot(actual)\n",
        "plt.plot(pred)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f10c029b2e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peDzBdcqkjzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}